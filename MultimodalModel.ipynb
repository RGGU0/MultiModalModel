{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "oeNZ8OIM_Aa1"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "#!pip install torchtext\n",
        "import torch, torchtext\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ch8wYq8qC0l_"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "#!pip install torcheval\n",
        "#from torcheval.metrics.functional import multiclass_f1_score\n",
        "from transformers import BertTokenizer, BertModel, AutoModel, AutoProcessor\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oLPEvpwQBGvV"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:32:25.313809Z",
          "iopub.status.busy": "2024-01-28T06:32:25.313121Z",
          "iopub.status.idle": "2024-01-28T06:32:25.318012Z",
          "shell.execute_reply": "2024-01-28T06:32:25.316933Z",
          "shell.execute_reply.started": "2024-01-28T06:32:25.313775Z"
        },
        "id": "9DtU-hxbBGvb",
        "papermill": {
          "duration": 0.082325,
          "end_time": "2023-08-30T20:42:44.894024",
          "exception": false,
          "start_time": "2023-08-30T20:42:44.811699",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "num_epochs = 25\n",
        "#image_size = 256\n",
        "image_size = 224"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "        Positional Encoding for Transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, dual_modality=False, dropout: float = 0.1, max_len: int = 5000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: dimension of model\n",
        "            dual_modality: when True, add a sequence of 0s or 1s depending on the modality\n",
        "            dropout: dropout rate\n",
        "            max_len: max length of sequence\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "        self.dual_modality = dual_modality\n",
        "        self.pe = self.pe.to(device)\n",
        "\n",
        "    def forward(self, x, is_first=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input tensor (bs, sqlen, emb)\n",
        "            is_first: True if the first modality, False if the second modality\n",
        "        \"\"\"\n",
        "        if self.dual_modality:\n",
        "            modality = torch.ones((x.shape[0], x.shape[1], 4), dtype=torch.float32).to(device) * (0 if is_first else 1)\n",
        "            x = x + self.pe[:, :x.size(1)]\n",
        "            x = self.dropout(x)\n",
        "            return torch.cat((x, modality), axis=-1)\n",
        "        else:\n",
        "            # x = (bs, sqlen, emb)  pe = (1, sqlen, emb)\n",
        "            x = x + self.pe[:, :x.size(1)]\n",
        "            return self.dropout(x)\n",
        "\n",
        "\n",
        "\n",
        "class CustomScaleDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    compute scale dot product attention\n",
        "\n",
        "    Query : given sentence that we focused on (decoder)\n",
        "    Key : every sentence to check relationship with Qeury(encoder)\n",
        "    Value : every sentence same with Key (encoder)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CustomScaleDotProductAttention, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v, text_mask, audio_mask, e=1e-12):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q: query (decoder)\n",
        "            k: key (encoder)\n",
        "            v: value (encoder)\n",
        "            text_mask: mask for text sequence\n",
        "            audio_mask: mask for audio sequence\n",
        "            e: epsilon value for masking\n",
        "        \"\"\"\n",
        "        # input is 4 dimension tensor\n",
        "        # [batch_size, head, length, d_tensor]\n",
        "        batch_size, head, length, d_tensor = k.size()\n",
        "\n",
        "        # 1. dot product Query with Key^T to compute similarity\n",
        "        k_t = k.transpose(2, 3)  # transpose\n",
        "        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
        "        ## score dimension: (batch, n_heads, length, length)\n",
        "\n",
        "        # 2. apply masking (opt)\n",
        "        padding_mask = torch.cat((text_mask, audio_mask), dim=1).unsqueeze(1)\n",
        "        ## padding_mask is now (batch, 1, seq_length)\n",
        "        score = score.masked_fill(padding_mask.unsqueeze(-1) == 0, -10_000) # padding_mask applied = (batch, 1, seq_length, 1)\n",
        "        score = score.masked_fill(padding_mask.unsqueeze(1) == 0, -10_000) # padding_mask applied = (batch, 1, 1, seq_length)\n",
        "\n",
        "        # 3. pass them softmax to make [0, 1] range\n",
        "        score = self.softmax(score)\n",
        "\n",
        "        # 4. multipy score with attention coefficients\n",
        "        text_lengths = torch.sum(text_mask, dim=1)\n",
        "        audio_lengths = torch.sum(audio_mask, dim=1)\n",
        "        ## text_length and audio_length dimension = (batch,)\n",
        "\n",
        "        total_lengths = text_lengths + audio_lengths\n",
        "        ## total_lengths dimension = (batch,)\n",
        "\n",
        "        text_coefficients = (total_lengths / (2*text_lengths)).unsqueeze(-1)\n",
        "        audio_coefficients = (total_lengths / (2*audio_lengths)).unsqueeze(-1)\n",
        "        ## text_coefficients and audio_coefficients dimension = (batch, 1)\n",
        "\n",
        "        text_weights = text_mask*text_coefficients\n",
        "        audio_weights = audio_mask*audio_coefficients\n",
        "        ## text_weights dimension = (batch, text_sequence_length)\n",
        "        ## audio_weights dimension = (batch, audio_sequence_length)\n",
        "\n",
        "        attention_coefficients = torch.cat((text_weights, audio_weights), dim=1).unsqueeze(1).unsqueeze(1)\n",
        "        ## attention_coefficients dimension = (batch, 1, 1, total_sequence_length)\n",
        "        score = score * attention_coefficients\n",
        "\n",
        "        # 5. multiply with Value\n",
        "        v = score @ v\n",
        "\n",
        "        return v, score\n",
        "\n",
        "\n",
        "class CustomMultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi Head Attention Class for Transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_head):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: dimension of model\n",
        "            n_head: number of heads\n",
        "        \"\"\"\n",
        "        super(CustomMultiHeadAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.attention = CustomScaleDotProductAttention()\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_concat = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, text_mask, audio_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q: query (decoder)\n",
        "            k: key (encoder)\n",
        "            v: value (encoder)\n",
        "            text_mask: mask for text sequence\n",
        "            audio_mask: mask for audio sequence\n",
        "        \"\"\"\n",
        "        # 1. dot product with weight matrices\n",
        "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
        "\n",
        "        # 2. split tensor by number of heads\n",
        "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
        "\n",
        "        # 3. do scale dot product to compute similarity\n",
        "        out, attention = self.attention(q, k, v, text_mask, audio_mask)\n",
        "\n",
        "        # 4. concat and pass to linear layer\n",
        "        out = self.concat(out)\n",
        "        out = self.w_concat(out)\n",
        "\n",
        "        # 5. visualize attention map\n",
        "        # TODO : we should implement visualization\n",
        "\n",
        "        return out\n",
        "\n",
        "    def split(self, tensor):\n",
        "        \"\"\"\n",
        "        split tensor by number of head\n",
        "\n",
        "        Args:\n",
        "            tensor: [batch_size, length, d_model]\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, length, d_model = tensor.size()\n",
        "\n",
        "        d_tensor = d_model // self.n_head\n",
        "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
        "        # it is similar with group convolution (split by number of heads)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def concat(self, tensor):\n",
        "        \"\"\"\n",
        "        inverse function of self.split(tensor : torch.Tensor)\n",
        "        Args:\n",
        "            tensor: [batch_size, head, length, d_tensor]\n",
        "        \"\"\"\n",
        "        batch_size, head, length, d_tensor = tensor.size()\n",
        "        d_model = head * d_tensor\n",
        "\n",
        "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
        "        return tensor\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Layer Normalization Class\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, eps=1e-12):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: dimension of model\n",
        "            eps: epsilon value for masking\n",
        "        \"\"\"\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input tensor\n",
        "        \"\"\"\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        var = x.var(-1, unbiased=False, keepdim=True)\n",
        "        # '-1' means last dimension.\n",
        "\n",
        "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        out = self.gamma * out + self.beta\n",
        "        return out\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise Feed Forward Layer\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: dimension of model\n",
        "            hidden: dimension of hidden layer\n",
        "            drop_prob: dropout rate\n",
        "        \"\"\"\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input tensor\n",
        "        \"\"\"\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "class CustomEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder Layer Class\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: dimension of model\n",
        "            ffn_hidden: dimension of hidden layer\n",
        "            n_head: number of heads\n",
        "            drop_prob: dropout rate\n",
        "        \"\"\"\n",
        "        super(CustomEncoderLayer, self).__init__()\n",
        "        self.attention = CustomMultiHeadAttention(d_model=d_model, n_head=n_head)\n",
        "        self.norm1 = LayerNorm(d_model=d_model)\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNorm(d_model=d_model)\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, text_mask, audio_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input tensor\n",
        "            text_mask: mask for text sequence\n",
        "            audio_mask: mask for audio sequence\n",
        "        \"\"\"\n",
        "        # 1. compute self attention\n",
        "        _x = x\n",
        "        x = self.attention(q=x, k=x, v=x, text_mask=text_mask, audio_mask=audio_mask)\n",
        "\n",
        "        # 2. add and norm\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + _x)\n",
        "\n",
        "        # 3. positionwise feed forward network\n",
        "        _x = x\n",
        "        x = self.ffn(x)\n",
        "\n",
        "        # 4. add and norm\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + _x)\n",
        "        return x\n",
        "\n",
        "class CustomEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder Class\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, ffn_hidden, n_head, n_layers, drop_prob):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: dimension of model\n",
        "            ffn_hidden: dimension of hidden layer\n",
        "            n_head: number of heads\n",
        "            n_layers: number of layers\n",
        "            drop_prob: dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([CustomEncoderLayer(d_model=d_model,ffn_hidden=ffn_hidden,n_head=n_head,drop_prob=drop_prob) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, embedding, text_mask, audio_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding: input tensor\n",
        "            text_mask: mask for text sequence\n",
        "            audio_mask: mask for audio sequence\n",
        "        \"\"\"\n",
        "        x = embedding\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, text_mask, audio_mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "aZMydzLDvSsL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb3IHh-sBGvc",
        "papermill": {
          "duration": 0.066364,
          "end_time": "2023-08-30T20:42:45.056849",
          "exception": false,
          "start_time": "2023-08-30T20:42:44.990485",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Import Libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "_kg_hide-output": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-01-28T06:26:58.336508Z",
          "iopub.status.busy": "2024-01-28T06:26:58.335538Z",
          "iopub.status.idle": "2024-01-28T06:26:58.792978Z",
          "shell.execute_reply": "2024-01-28T06:26:58.791995Z",
          "shell.execute_reply.started": "2024-01-28T06:26:58.336446Z"
        },
        "id": "dwT2SAWxBGve",
        "outputId": "6ac2adb7-4ed5-414c-dcc9-6e7a7b3e3631",
        "papermill": {
          "duration": 3.357763,
          "end_time": "2023-08-30T20:42:48.481953",
          "exception": false,
          "start_time": "2023-08-30T20:42:45.12419",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import itertools\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "A7Qn9dhG_Aa_",
        "outputId": "9b6898ad-769d-4325-e4d2-891caddcca1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp7EPCxdXbL_",
        "outputId": "27f0e92b-b11a-4190-84f8-7599f7418cd0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:23:14.494284Z",
          "iopub.status.busy": "2024-01-28T06:23:14.493658Z",
          "iopub.status.idle": "2024-01-28T06:23:14.594063Z",
          "shell.execute_reply": "2024-01-28T06:23:14.593009Z",
          "shell.execute_reply.started": "2024-01-28T06:23:14.494248Z"
        },
        "id": "OFuWq6QPBGvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "126da3b9-993b-46b8-90b0-2be3a7f3123d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "'''df2 = pd.read_csv('D:\\\\IndianaReports\\\\indiana_projections.csv')\n",
        "df1 = pd.read_csv('D:\\\\IndianaReports\\\\indiana_reports.csv')'''\n",
        "''''df2 = pd.read_csv('/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv')\n",
        "df1 = pd.read_csv('/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv')\n",
        "'''\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/LangsImgDatasetForDiploma/indiana_projections.csv')\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/LangsImgDatasetForDiploma/indiana_reports.csv')\n",
        "\n",
        "df1.loc[df1['Problems'].str.contains('Lung'),'Problems']='Lung'\n",
        "df1 = df1.loc[df1['Problems'].isin(['Lung','normal'])]\n",
        "df1 = df1[:50]\n",
        "len(df1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-01-28T06:23:17.189467Z",
          "iopub.status.busy": "2024-01-28T06:23:17.188516Z",
          "iopub.status.idle": "2024-01-28T06:23:26.789353Z",
          "shell.execute_reply": "2024-01-28T06:23:26.788368Z",
          "shell.execute_reply.started": "2024-01-28T06:23:17.189427Z"
        },
        "id": "wnWzBnpXBGvg",
        "outputId": "0085a29b-3cfc-4530-b3d1-7acd07d9e7ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       image  \\\n",
            "0     1_IM-0001-4001.dcm.png   \n",
            "1     1_IM-0001-3001.dcm.png   \n",
            "2  5_IM-2117-1003002.dcm.png   \n",
            "3  5_IM-2117-1004003.dcm.png   \n",
            "4     6_IM-2192-1001.dcm.png   \n",
            "\n",
            "                                             caption  number_of_words problem  \n",
            "0  The cardiac silhouette and mediastinum size ar...               34  normal  \n",
            "1  The cardiac silhouette and mediastinum size ar...               34  normal  \n",
            "2  The cardiomediastinal silhouette and pulmonary...               42    Lung  \n",
            "3  The cardiomediastinal silhouette and pulmonary...               42    Lung  \n",
            "4  Heart size and mediastinal contour are within ...               32  normal  \n"
          ]
        }
      ],
      "source": [
        "images_captions_df = pd.DataFrame({'image': [],\n",
        "                                    'caption': [],'number_of_words':[],'problem':[]})\n",
        "for i in range(len(df2)):\n",
        "    uid = df2.iloc[i]['uid']\n",
        "    image = df2.iloc[i]['filename']\n",
        "    index = df1.loc[df1['uid'] ==uid]\n",
        "\n",
        "    if not index.empty:\n",
        "        index = index.index[0]\n",
        "        caption = df1.loc[index]['findings']\n",
        "        problem = df1.loc[index]['Problems']\n",
        "        number_of_words = len(str(caption).split())\n",
        "\n",
        "        if type(caption) == float:\n",
        "                continue\n",
        "        images_captions_df = pd.concat([images_captions_df, pd.DataFrame([{'image': image, 'caption': caption ,'number_of_words':number_of_words,'problem':problem}])], ignore_index=True)\n",
        "\n",
        "images_captions_df[\"number_of_words\"] =  images_captions_df[\"caption\"].apply(lambda text: len(str(text).split()))\n",
        "images_captions_df['number_of_words'] = images_captions_df['number_of_words'].astype(int)\n",
        "\n",
        "print(images_captions_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-01-28T06:23:29.958866Z",
          "iopub.status.busy": "2024-01-28T06:23:29.958519Z",
          "iopub.status.idle": "2024-01-28T06:23:30.562072Z",
          "shell.execute_reply": "2024-01-28T06:23:30.56112Z",
          "shell.execute_reply.started": "2024-01-28T06:23:29.958839Z"
        },
        "id": "yos3knizBGvg",
        "outputId": "a889e11a-486b-462d-a73d-a594904f2520",
        "papermill": {
          "duration": 55.937704,
          "end_time": "2023-08-30T20:43:44.953316",
          "exception": false,
          "start_time": "2023-08-30T20:42:49.015612",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       image  \\\n",
            "89  100_IM-0002-2001.dcm.png   \n",
            "65   72_IM-2280-1001.dcm.png   \n",
            "35   36_IM-1776-2001.dcm.png   \n",
            "24   25_IM-1024-2001.dcm.png   \n",
            "79   87_IM-2390-2001.dcm.png   \n",
            "\n",
            "                                              caption  number_of_words problem  \n",
            "89  Both lungs are clear and expanded. Heart and m...               10  normal  \n",
            "65  Lungs are clear without focal consolidation, e...               19  normal  \n",
            "35  The lungs are clear bilaterally. Specifically,...               30  normal  \n",
            "24  The heart is within normal limits in size. Sur...               48    Lung  \n",
            "79  No focal airspace disease, pleural effusion or...               18  normal  \n",
            "72 14 4\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "image_folder = '/content/drive/MyDrive/LangsImgDatasetForDiploma/images_normalized'\n",
        "image_filenames = images_captions_df.image.values\n",
        "\n",
        "# image_filenames = image_filenames[0:100]\n",
        "\n",
        "train_captions,test_val_captions =train_test_split(images_captions_df, test_size = 0.2)\n",
        "test_captions, val_captions =train_test_split(test_val_captions, test_size = 0.2)\n",
        "\n",
        "print(train_captions.head())\n",
        "print(len(train_captions),len(test_captions),len(val_captions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZepJHNzT_AbD",
        "outputId": "11db4ea4-c467-4803-dfe5-68695de74191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       image  \\\n",
              "89  100_IM-0002-2001.dcm.png   \n",
              "65   72_IM-2280-1001.dcm.png   \n",
              "35   36_IM-1776-2001.dcm.png   \n",
              "24   25_IM-1024-2001.dcm.png   \n",
              "79   87_IM-2390-2001.dcm.png   \n",
              "..                       ...   \n",
              "34   36_IM-1776-1001.dcm.png   \n",
              "78   87_IM-2390-1001.dcm.png   \n",
              "76   86_IM-2380-3001.dcm.png   \n",
              "41   41_IM-2055-2001.dcm.png   \n",
              "1     1_IM-0001-3001.dcm.png   \n",
              "\n",
              "                                              caption  number_of_words problem  \n",
              "89  Both lungs are clear and expanded. Heart and m...               10  normal  \n",
              "65  Lungs are clear without focal consolidation, e...               19  normal  \n",
              "35  The lungs are clear bilaterally. Specifically,...               30  normal  \n",
              "24  The heart is within normal limits in size. Sur...               48    Lung  \n",
              "79  No focal airspace disease, pleural effusion or...               18  normal  \n",
              "..                                                ...              ...     ...  \n",
              "34  The lungs are clear bilaterally. Specifically,...               30  normal  \n",
              "78  No focal airspace disease, pleural effusion or...               18  normal  \n",
              "76  Heart size is within normal limits. No focal a...               15  normal  \n",
              "41  The cardiomediastinal silhouette is stable in ...               57    Lung  \n",
              "1   The cardiac silhouette and mediastinum size ar...               34  normal  \n",
              "\n",
              "[72 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-568e2afd-f7fa-44f2-b57b-3f00368deea9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>caption</th>\n",
              "      <th>number_of_words</th>\n",
              "      <th>problem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>100_IM-0002-2001.dcm.png</td>\n",
              "      <td>Both lungs are clear and expanded. Heart and m...</td>\n",
              "      <td>10</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>72_IM-2280-1001.dcm.png</td>\n",
              "      <td>Lungs are clear without focal consolidation, e...</td>\n",
              "      <td>19</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>36_IM-1776-2001.dcm.png</td>\n",
              "      <td>The lungs are clear bilaterally. Specifically,...</td>\n",
              "      <td>30</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25_IM-1024-2001.dcm.png</td>\n",
              "      <td>The heart is within normal limits in size. Sur...</td>\n",
              "      <td>48</td>\n",
              "      <td>Lung</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>87_IM-2390-2001.dcm.png</td>\n",
              "      <td>No focal airspace disease, pleural effusion or...</td>\n",
              "      <td>18</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>36_IM-1776-1001.dcm.png</td>\n",
              "      <td>The lungs are clear bilaterally. Specifically,...</td>\n",
              "      <td>30</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>87_IM-2390-1001.dcm.png</td>\n",
              "      <td>No focal airspace disease, pleural effusion or...</td>\n",
              "      <td>18</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>86_IM-2380-3001.dcm.png</td>\n",
              "      <td>Heart size is within normal limits. No focal a...</td>\n",
              "      <td>15</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>41_IM-2055-2001.dcm.png</td>\n",
              "      <td>The cardiomediastinal silhouette is stable in ...</td>\n",
              "      <td>57</td>\n",
              "      <td>Lung</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1_IM-0001-3001.dcm.png</td>\n",
              "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
              "      <td>34</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>72 rows Ã— 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-568e2afd-f7fa-44f2-b57b-3f00368deea9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-568e2afd-f7fa-44f2-b57b-3f00368deea9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-568e2afd-f7fa-44f2-b57b-3f00368deea9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-85c93e9b-9053-4e40-b1b7-607728ee8e2d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-85c93e9b-9053-4e40-b1b7-607728ee8e2d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-85c93e9b-9053-4e40-b1b7-607728ee8e2d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_captions",
              "summary": "{\n  \"name\": \"train_captions\",\n  \"rows\": 72,\n  \"fields\": [\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 72,\n        \"samples\": [\n          \"87_IM-2390-2001.dcm.png\",\n          \"79_IM-2329-1001.dcm.png\",\n          \"89_IM-2402-1001.dcm.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 43,\n        \"samples\": [\n          \"The heart size and pulmonary vascularity appear within normal limits. The lungs are free of focal airspace disease. No pleural effusion or pneumothorax is seen.\",\n          \"The XXXX examination consists of frontal and lateral radiographs of the chest. The cardiomediastinal contours are within normal limits. Pulmonary vascularity is within normal limits. No focal consolidation, pleural effusion, or pneumothorax identified. Deformity of the right clavicle related to remote XXXX is again seen. Visualized upper abdomen grossly unremarkable.\",\n          \"2 images. Heart size and pulmonary vascular engorgement appear within limits of normal. Mediastinal contour is unremarkable. No focal consolidation, pleural effusion, or pneumothorax identified. No convincing acute bony findings.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_of_words\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 10,\n        \"max\": 57,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          36,\n          50,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"problem\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Lung\",\n          \"normal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "train_captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "execution": {
          "iopub.execute_input": "2024-01-28T06:35:52.549609Z",
          "iopub.status.busy": "2024-01-28T06:35:52.549208Z",
          "iopub.status.idle": "2024-01-28T06:35:52.780194Z",
          "shell.execute_reply": "2024-01-28T06:35:52.779321Z",
          "shell.execute_reply.started": "2024-01-28T06:35:52.549579Z"
        },
        "id": "-CIBbEVRBGvi",
        "outputId": "81a7ee75-6b5f-417a-d4e8-871eb20eab1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(224, 224, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# test , not important\n",
        "def image_preprocessing(image_path,target_size):\n",
        "   # Load the image using PIL\n",
        "    img = cv2.imread(image_path)\n",
        "    # Resize the image using PIL's resize method\n",
        "    img = cv2.resize(img,(target_size,target_size), )\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "def display_image(image_array):\n",
        "    # Display the image using matplotlib\n",
        "    plt.imshow(image_array)\n",
        "    plt.axis('off')  # Turn off axis labels\n",
        "    plt.show()\n",
        "#image = image_preprocessing(\"E:/LangsImgDatasetForDiploma/images_normalized/1000_IM-0003-1001.dcm.png\",image_size)\n",
        "#image = image_preprocessing(\"/kaggle/input/chest-xrays-indiana-university/images/images_normalized/1000_IM-0003-1001.dcm.png\",image_size)\n",
        "image = image_preprocessing(\"/content/drive/MyDrive/LangsImgDatasetForDiploma/images_normalized/1000_IM-0003-1001.dcm.png\",image_size)\n",
        "\n",
        "print(image.shape)\n",
        "display_image(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Kyv_SarCtLX",
        "outputId": "a54d5e98-fc15-4a04-96f4-6509d67c8dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "\"\"\"\n",
        "For the Argumentative Component Classification (ACC) task, we need to remove the 'O' label from the dataset.\n",
        "\"\"\"\n",
        "MODEL_NUM_LABELS = 3\n",
        "REMOVE_OTHER = True\n",
        "OTHER_LABEL = 'O'\n",
        "\n",
        "if REMOVE_OTHER:\n",
        "    MODEL_NUM_LABELS = 2\n",
        "\n",
        "TEXT_MODEL_CARD = 'bert-base-uncased'\n",
        "AUDIO_MODEL_CARD = 'facebook/wav2vec2-base-960h'\n",
        "\n",
        "LABEL_2_ID = {\n",
        "    'normal': 0,\n",
        "    'Lung': 1\n",
        "}\n",
        "\n",
        "ID_2_LABEL = {\n",
        "    0: 'normal',\n",
        "    1: 'Lung'\n",
        "}\n",
        "\n",
        "\n",
        "EMBEDDING_DIM = 768\n",
        "BATCH_SIZE = 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:36:52.666563Z",
          "iopub.status.busy": "2024-01-28T06:36:52.666188Z",
          "iopub.status.idle": "2024-01-28T06:36:52.67664Z",
          "shell.execute_reply": "2024-01-28T06:36:52.675681Z",
          "shell.execute_reply.started": "2024-01-28T06:36:52.666532Z"
        },
        "id": "NQPjCf79BGvj",
        "papermill": {
          "duration": 0.119935,
          "end_time": "2023-08-30T20:43:45.402113",
          "exception": false,
          "start_time": "2023-08-30T20:43:45.282178",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Get the number of available processors\n",
        "num_processors = os.cpu_count()\n",
        "\n",
        "# Example dataset class\n",
        "class ImageTextDataset(Dataset):\n",
        "    def __init__(self, image_filenames, captions, problems, image_size=256):\n",
        "        self.image_filenames = image_filenames\n",
        "        self.captions = captions\n",
        "        self.problems = problems\n",
        "        self.transform = transforms.Compose([\n",
        "                                                 # Resize images to a consistent size\n",
        "                                                #transforms.ToTensor(),\n",
        "                                                transforms.ToTensor(),          # Convert images to tensors\n",
        "                                                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
        "                                            ])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #image_path = \"E:/LangsImgDatasetForDiploma/images_normalized/\"+image_filenames[index]\n",
        "        #image_path = \"/kaggle/input/chest-xrays-indiana-university/images/images_normalized/\"+image_filenames[index]\n",
        "        image_path = \"/content/drive/MyDrive/LangsImgDatasetForDiploma/images_normalized/\"+image_filenames[index]\n",
        "\n",
        "        image = image_preprocessing(image_path,image_size ) #\n",
        "        image = list(np.array(self.transform(image)))#.permute(2,1,0)\n",
        "        text = self.captions[index]\n",
        "        problem = LABEL_2_ID[self.problems[index]]\n",
        "\n",
        "        return image, text, problem\n",
        "\n",
        "# Create an instance of the dataset\n",
        "\n",
        "train_dataset = ImageTextDataset(train_captions.image.values, train_captions.caption.values, train_captions.problem.values, image_size)\n",
        "test_dataset = ImageTextDataset(test_captions.image.values, test_captions.caption.values, test_captions.problem.values, image_size)\n",
        "val_dataset = ImageTextDataset(val_captions.image.values, val_captions.caption.values, val_captions.problem.values, image_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kGZnJchw_AbF",
        "outputId": "c7d2a79e-c0b4-4093-e13a-7e7e8e74f135",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 224, 224)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "np.shape(train_dataset[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "execution": {
          "iopub.execute_input": "2024-01-28T06:36:53.547354Z",
          "iopub.status.busy": "2024-01-28T06:36:53.547013Z",
          "iopub.status.idle": "2024-01-28T06:36:53.605329Z",
          "shell.execute_reply": "2024-01-28T06:36:53.604269Z",
          "shell.execute_reply.started": "2024-01-28T06:36:53.547327Z"
        },
        "id": "JgnlE-u-BGvj",
        "outputId": "b8f134d2-b175-49db-cfb7-d71212c43107",
        "papermill": {
          "duration": 0.182376,
          "end_time": "2023-08-30T20:43:45.691164",
          "exception": false,
          "start_time": "2023-08-30T20:43:45.508788",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-764780929f4c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "print(train_dataset. __getitem__(0)[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "V6y_jyBsCMgF"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(dataset, batch_size):\n",
        "    \"\"\"\n",
        "    Create a DataLoader object from the given dataset with the given batch size\n",
        "    Args:\n",
        "        dataset: dataset to use\n",
        "        batch_size: batch size to use\n",
        "    \"\"\"\n",
        "    def pack_fn(batch):\n",
        "        \"\"\"\n",
        "        Function to pad the audio features and create the attention mask\n",
        "        \"\"\"\n",
        "        #print('batch\\r\\n',batch)\n",
        "        texts = [x[1] for x in batch]\n",
        "        ''''for x in batch:\n",
        "            print(type(x[0]),x[0].shape)\n",
        "            print('x[0]\\r\\n',x[0])'''\n",
        "        audio_features = [x[0] for x in batch]\n",
        "        labels = torch.tensor([x[2] for x in batch])\n",
        "        return texts, audio_features, labels\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pack_fn)\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DlxbSNhLt31r"
      },
      "outputs": [],
      "source": [
        "# Ð­Ñ‚Ð¾Ñ‚ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚\n",
        "train_dataloader = create_dataloader(train_dataset, batch_size)\n",
        "val_dataloader = create_dataloader(val_dataset, batch_size)\n",
        "test_dataloader = create_dataloader(test_dataset, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM6uH9GorKC4"
      },
      "outputs": [],
      "source": [
        "next(iter(train_dataloader))[1][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4KBDBPZt-F8"
      },
      "outputs": [],
      "source": [
        "next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCBjI9GFBGvk",
        "papermill": {
          "duration": 0.106168,
          "end_time": "2023-08-30T20:43:45.905225",
          "exception": false,
          "start_time": "2023-08-30T20:43:45.799057",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Display Few Image with Caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-01-28T06:36:57.123242Z",
          "iopub.status.busy": "2024-01-28T06:36:57.122831Z",
          "iopub.status.idle": "2024-01-28T06:36:58.037891Z",
          "shell.execute_reply": "2024-01-28T06:36:58.036368Z",
          "shell.execute_reply.started": "2024-01-28T06:36:57.12321Z"
        },
        "id": "xBuZqG95BGvk",
        "papermill": {
          "duration": 5.115808,
          "end_time": "2023-08-30T20:43:51.127194",
          "exception": false,
          "start_time": "2023-08-30T20:43:46.011386",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Fetch a batch from the dataloader\n",
        "batch_texts, batch_images,  batch_problems = next(iter(train_dataloader))\n",
        "\n",
        "for index in range(batch_images[0].shape[0]):\n",
        "    # Fetch the image and text for the chosen index\n",
        "    image_to_display = batch_images[index]\n",
        "    text_to_display = batch_texts[index]\n",
        "    problem_to_display = batch_problems[index]\n",
        "\n",
        "    # Display the problem\n",
        "    print(\"Problem:\", problem_to_display, \"\\n\\n\\n\")\n",
        "\n",
        "    # Convert the image tensor to a NumPy array\n",
        "    image_to_display_np = image_to_display.permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Display the image using matplotlib\n",
        "    plt.imshow(image_to_display_np)\n",
        "    plt.axis('off')  # Turn off axis labels\n",
        "    plt.show()\n",
        "\n",
        "    # Display the corresponding text\n",
        "    print(\"Caption:\", text_to_display, \"\\n\\n\\n\")\n",
        "    if index>=5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XixhCkkrCrCd"
      },
      "source": [
        "# # Create a DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KsFjNdA7Cwkv"
      },
      "outputs": [],
      "source": [
        "class TextModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Class for the text-only model\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer, embedder, head):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tokenizer: tokenizer to use\n",
        "            embedder: embedder to use\n",
        "            head: head to use\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embedder = embedder\n",
        "        self.head = head\n",
        "    def forward(self, texts, audio_features):\n",
        "        \"\"\", audio_attention\n",
        "        Forward pass of the model\n",
        "        Args:\n",
        "            texts: texts to use\n",
        "            audio_features: audio features to use\n",
        "            audio_attentions: audio attentions to use\n",
        "        \"\"\"\n",
        "        tokenizer_output = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n",
        "        embedder_output = self.embedder(**tokenizer_output, output_hidden_states=True)\n",
        "        text_features = embedder_output['last_hidden_state']\n",
        "        print('text_features',text_features.shape)\n",
        "        print('text_attentions',tokenizer_output.attention_mask.shape)\n",
        "        print('text_features', embedder_output['hidden_states'][8].shape)\n",
        "\n",
        "        # pooling transformer output\n",
        "        text_features_sum = (text_features * tokenizer_output.attention_mask.unsqueeze(-1)).sum(axis=1)\n",
        "        text_features_pooled = text_features_sum / tokenizer_output.attention_mask.sum(axis=1).unsqueeze(-1)\n",
        "\n",
        "        print('text_features_pooled',text_features_pooled.shape)\n",
        "        return self.head(text_features_pooled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vr5i7iM_AbJ"
      },
      "outputs": [],
      "source": [
        "head_hidden_dimension=224\n",
        "text_only_head = nn.Sequential(\n",
        "        nn.Linear(EMBEDDING_DIM, head_hidden_dimension),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(head_hidden_dimension, MODEL_NUM_LABELS)\n",
        "    ).to(device)\n",
        "text_only = TextModel(tokenizer, embedder, text_only_head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYo_i01v_AbK"
      },
      "outputs": [],
      "source": [
        "b = next(iter(train_dataloader))\n",
        "texts = b[0]\n",
        "pixel_values = b[1]\n",
        "text_only.forward(texts, pixel_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6qd0DHbR_AbK"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTImageProcessor, ViTModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "cldAIkbL_AbK"
      },
      "outputs": [],
      "source": [
        "class ViTForImageClassification(nn.Module):\n",
        "    def __init__(self, head, num_labels=MODEL_NUM_LABELS):\n",
        "        super(ViTForImageClassification, self).__init__()\n",
        "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
        "        self.num_labels = num_labels\n",
        "        self.head = head\n",
        "    def forward(self, texts, pixel_values):\n",
        "        #print('texts',texts)\n",
        "        #print('pixel_values',pixel_values)\n",
        "        #batch_size, num_channels, height, width\n",
        "        pixel_values_0 = torch.tensor(pixel_values).clone().detach()\n",
        "        print('pixel_values_0',pixel_values_0.shape)\n",
        "        outputs = self.vit(pixel_values=pixel_values_0)\n",
        "        print('type outputs', type(outputs))\n",
        "\n",
        "        #outputs_att = self.get_output_attentions(pixel_values_0)\n",
        "        outputs_att = self.vit(pixel_values_0, output_attentions=True, interpolate_pos_encoding=True)\n",
        "        attentions = outputs_att.attentions\n",
        "        print('attentions',len(attentions))\n",
        "        #attention_img(outputs_att, pixel_values_0)\n",
        "\n",
        "        print('last_hidden_states',outputs.pooler_output.shape)\n",
        "        last_hidden_states = outputs.pooler_output\n",
        "        print('last_hidden_states_0',last_hidden_states.shape)\n",
        "\n",
        "        output = self.dropout(last_hidden_states)\n",
        "        print('output',output.shape)\n",
        "        return self.head(output)\n",
        "    def get_output_attentions(self, pixel_values):\n",
        "        return self.vit(torch.tensor(pixel_values).clone().detach(), output_attentions=True, interpolate_pos_encoding=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3, 224, 224)"
      ],
      "metadata": {
        "id": "XS8hgHwXCzA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.FloatTensor(12, 196)\n",
        "a.reshape(12, 14, 14).shape"
      ],
      "metadata": {
        "id": "j8__ykB73ucw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input.flatten(start_dim=2).shape"
      ],
      "metadata": {
        "id": "EjU8Dacp9a8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input.reshape(4, 3, 196).shape"
      ],
      "metadata": {
        "id": "TGb3stNW_dLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pool of square window of size=3, stride=2\n",
        "m = nn.MaxPool1d(3, stride=2)\n",
        "# pool of non-square window\n",
        "#m = nn.MaxPool2d((3, 2), stride=(2, 1))\n",
        "input = torch.randn(12, 196)\n",
        "output = m(input)\n",
        "print(input.shape,output.shape)"
      ],
      "metadata": {
        "id": "Lq22h1lr5Fl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_img(outputs, pixel_val):\n",
        "    attentions = outputs.attentions[-1] # we are only interested in the attention maps of the last layer\n",
        "    nh = attentions.shape[1] # number of head\n",
        "    print('attentions.shape attention_img',attentions.shape)\n",
        "    # we keep only the output patch attention\n",
        "    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
        "    print('attentions.reshape attention_img',attentions.shape)\n",
        "    pixel_val = torch.tensor(pixel_val[0])\n",
        "\n",
        "    threshold = 0.6\n",
        "    w_featmap = pixel_val.shape[-2] // batch_size\n",
        "    h_featmap = pixel_val.shape[-1] // batch_size\n",
        "\n",
        "    w_featmap = 14\n",
        "    h_featmap = 14\n",
        "\n",
        "    # we keep only a certain percentage of the mass\n",
        "    val, idx = torch.sort(attentions)\n",
        "    val /= torch.sum(val, dim=1, keepdim=True)\n",
        "    cumval = torch.cumsum(val, dim=1)\n",
        "    th_attn = cumval > (1 - threshold)\n",
        "    idx2 = torch.argsort(idx)\n",
        "    for head in range(nh):\n",
        "        th_attn[head] = th_attn[head][idx2[head]]\n",
        "    th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()\n",
        "    # interpolate\n",
        "    th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=batch_size, mode=\"nearest\")[0].cpu().numpy()\n",
        "\n",
        "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
        "    attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=batch_size, mode=\"nearest\")[0].cpu()\n",
        "    attentions = attentions.detach().numpy()\n",
        "\n",
        "    # show and save attentions heatmaps\n",
        "    output_dir = '.'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    #torchvision.utils.save_image(torchvision.utils.make_grid(pixel_val, normalize=True, scale_each=True), os.path.join(output_dir, \"img.png\"))\n",
        "    for j in range(nh):\n",
        "        fname = os.path.join(output_dir, \"attn-head\" + str(j) + \".png\")\n",
        "        plt.figure()\n",
        "        plt.imshow(attentions[j])\n",
        "        plt.imsave(fname=fname, arr=attentions[j], format='png')\n",
        "        #print(f\"{fname} saved.\")"
      ],
      "metadata": {
        "id": "BLq4ZnwVwIRf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTForImageClassification(nn.Module):\n",
        "    def __init__(self, head, num_labels=MODEL_NUM_LABELS):\n",
        "        super(ViTForImageClassification, self).__init__()\n",
        "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
        "        self.num_labels = num_labels\n",
        "        self.head = head\n",
        "        self.ln = LayerNorm(EMBEDDING_DIM)\n",
        "    def forward(self, texts, pixel_values):\n",
        "        #print('texts',texts)\n",
        "        #print('pixel_values',pixel_values)\n",
        "        #batch_size, num_channels, height, width\n",
        "        pixel_values_0 = torch.tensor(pixel_values)\n",
        "        print(pixel_values_0.shape)\n",
        "        outputs = self.vit(pixel_values=pixel_values_0)\n",
        "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
        "\n",
        "        outputs_att = self.get_output_attentions(pixel_values_0)\n",
        "        attention = outputs_att.attentions[-1]\n",
        "        nh = attention.shape[1] # number of head\n",
        "\n",
        "\n",
        "        # we keep only the output patch attention\n",
        "        attention = attention[0, :, 0, 1:].reshape(nh, -1)\n",
        "        attention = attention.reshape(nh, 14, 14)\n",
        "        attention = nn.functional.interpolate(attention.unsqueeze(0), scale_factor=batch_size, mode=\"nearest\")[0].cpu()\n",
        "        attention_img(outputs_att, pixel_values_0)\n",
        "        print(attention.shape)\n",
        "\n",
        "        print(output.shape)\n",
        "        #output = self.ln(pixel_values_0 + output)\n",
        "        output_sum = (output * attention).sum(axis=1)\n",
        "        output_pooled = output_sum / attention.sum(axis=1).unsqueeze(-1)\n",
        "\n",
        "        return self.head(output)\n",
        "    def get_output_attentions(self, pixel_values):\n",
        "        return self.vit(torch.tensor(pixel_values), output_attentions=True, interpolate_pos_encoding=True)"
      ],
      "metadata": {
        "id": "XjeS_I-Mvdoy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBCjDvX4_AbL"
      },
      "outputs": [],
      "source": [
        "type(torch.tensor(pixel_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4njgneHM_AbL"
      },
      "outputs": [],
      "source": [
        "head_hidden_dimension=224\n",
        "img_only_head = nn.Sequential(\n",
        "        nn.Linear(EMBEDDING_DIM, head_hidden_dimension),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(head_hidden_dimension, MODEL_NUM_LABELS)\n",
        "    ).to(device)\n",
        "model = ViTForImageClassification(img_only_head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P74asz2V_AbL"
      },
      "outputs": [],
      "source": [
        "b = next(iter(train_dataloader))\n",
        "texts = b[0]\n",
        "pixel_values = b[1]\n",
        "img_only.forward(texts, pixel_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92sk0iq0_AbM"
      },
      "outputs": [],
      "source": [
        "img_only.forward(texts, pixel_values)\n",
        "outputs = model.get_output_attentions(pixel_values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('pooler_output\\n',outputs.pooler_output)\n",
        "attentions = outputs.attentions\n",
        "print('attentions\\n',attentions[0][0][0][0])\n",
        "print('len attention\\n',len(attentions), len(attentions[0]), len(attentions[0][0]), len(attentions[0][0][0]), len(attentions[0][0][0][0]), len(attentions[0][0][0][0]))\n",
        "print('(batch_size, num_heads, sequence_length, sequence_length)')"
      ],
      "metadata": {
        "id": "YGrzmoiBJ9Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_to_display = attentions[0][0][6]\n",
        "image_to_display_np = image_to_display.detach().numpy()\n",
        "\n",
        "# Display the image using matplotlib\n",
        "plt.imshow(image_to_display_np)\n",
        "plt.axis('off')  # Turn off axis labels\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SvAbNZOcNq6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_only.forward(texts, pixel_values)\n",
        "outputs = model.get_output_attentions(pixel_values)\n",
        "attentions = outputs.attentions\n",
        "image_to_display = attentions[0][0][6]\n",
        "image_to_display_np = image_to_display.detach().numpy()\n",
        "# Display the image using matplotlib\n",
        "plt.imshow(image_to_display_np)\n",
        "plt.axis('off')  # Turn off axis labels\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "joeaWIMvQqWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR4G9WJw_AbM"
      },
      "outputs": [],
      "source": [
        "attentions = outputs.attentions[-1] # we are only interested in the attention maps of the last layer\n",
        "nh = attentions.shape[1] # number of head\n",
        "\n",
        "# we keep only the output patch attention\n",
        "attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
        "print(attentions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(4, 79)\n",
        "a = a[:, None, :]\n",
        "b = torch.randn(4, 3, 196)\n",
        "print(a.shape, b.shape)"
      ],
      "metadata": {
        "id": "jx9wjC_hNsKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b.flatten(1).shape"
      ],
      "metadata": {
        "id": "euzXrU0h_aQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dOET41r1_Abc"
      },
      "outputs": [],
      "source": [
        "class CSA(nn.Module):\n",
        "    \"\"\"\n",
        "    Class for the multimodal transformer model\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer, embedder, transformer, head, hidden_state_index=8):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tokenizer: tokenizer to use\n",
        "            embedder: embedder to use\n",
        "            transformer: transformer to use\n",
        "            head: head to use\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        #self.pos_encoder = PositionalEncoding(EMBEDDING_DIM, dual_modality=False)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embedder = embedder\n",
        "        self.transformer = transformer\n",
        "        self.head = head\n",
        "        self.hidden_state_index = hidden_state_index\n",
        "\n",
        "    def forward(self, texts, pixel_values):\n",
        "        \"\"\"\n",
        "        Forward pass of the model\n",
        "        Args:\n",
        "            texts: texts to use\n",
        "            audio_features: audio features to use\n",
        "            audio_attentions: audio attentions to use\n",
        "        \"\"\"\n",
        "        tokenizer_output = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n",
        "        embedder_output = self.embedder(**tokenizer_output, output_hidden_states=True)\n",
        "        if self.hidden_state_index == -1:\n",
        "            text_features = embedder_output['last_hidden_state']\n",
        "        else:\n",
        "            assert self.hidden_state_index < len(embedder_output['hidden_states']), f'hidden_state_index must be between 0 and {len(embedder_output[\"hidden_states\"])}'\n",
        "            text_features = embedder_output['hidden_states'][self.hidden_state_index]\n",
        "        text_features_sum = (text_features * tokenizer_output.attention_mask.unsqueeze(-1)).sum(axis=1)\n",
        "        text_features_pooled = text_features_sum / tokenizer_output.attention_mask.sum(axis=1).unsqueeze(-1)\n",
        "        text_attentions = tokenizer_output.attention_mask\n",
        "\n",
        "        pixel_values_0 = torch.tensor(pixel_values)\n",
        "\n",
        "        outputs_att = self.vit(pixel_values_0, output_attentions=True, interpolate_pos_encoding=True)\n",
        "\n",
        "        audio_attentions = outputs_att.attentions\n",
        "\n",
        "        ''''audio_attentions = outputs_att.attentions[-1] # we are only interested in the attention maps of the last layer\n",
        "        nh = audio_attentions.shape[1] # number of head\n",
        "        audio_attentions = audio_attentions[0, :, 0, 1:].reshape(nh, -1)'''\n",
        "\n",
        "        print('audio_attentions[0]',audio_attentions[0].shape, type(audio_attentions), type(audio_attentions[0]), 'len audio_attentions', len(audio_attentions))#4 *\n",
        "        print('text_attentions', text_attentions.shape, type(text_attentions))#4 *\n",
        "\n",
        "        ''''concatenated_attentions = torch.cat((text_attentions,audio_attentions[0].flatten(1)), dim=1)\n",
        "        for tth in audio_attentions[1:]:\n",
        "          concatenated_attentions = torch.cat((concatenated_attentions, tth.flatten(1)), dim=1)'''\n",
        "        m = nn.Linear(465708, 197)\n",
        "        audio_attentions = m(torch.tensor(audio_attentions[0]).flatten(1))\n",
        "\n",
        "        concatenated_attentions = torch.cat((text_attentions, audio_attentions), dim=1)\n",
        "\n",
        "        print('concatenated_attentions',concatenated_attentions.shape)\n",
        "\n",
        "        #audio_features = outputs_att.pooler_output\n",
        "        audio_features = outputs_att.last_hidden_state\n",
        "\n",
        "        #audio_features = self.pos_encoder(audio_features)\n",
        "\n",
        "        print('text_features',text_features.shape, 'audio_features', audio_features.shape)\n",
        "\n",
        "        concatenated_features = torch.cat((text_features, audio_features), dim=1)\n",
        "\n",
        "        print('concatenated_features',concatenated_features.shape)\n",
        "\n",
        "        print('text_attentions',text_attentions.shape, 'audio_attentions', audio_attentions.shape)\n",
        "        transformer_output = self.transformer(concatenated_features, text_attentions, audio_attentions)\n",
        "\n",
        "        # pooling of transformer output\n",
        "        transformer_output_sum = (transformer_output * concatenated_attentions.unsqueeze(-1)).sum(axis=1) # 4 d d 768 d\n",
        "        transformer_output_pooled = transformer_output_sum / concatenated_attentions.sum(axis=1).unsqueeze(-1) # 4 d 768 d\n",
        "        return self.head(transformer_output_pooled) #4, 768\n",
        "    def get_output_attentions(self, pixel_values):\n",
        "        return self.vit(torch.tensor(pixel_values), output_attentions=True, interpolate_pos_encoding=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CSA(nn.Module):\n",
        "    \"\"\"\n",
        "    Class for the multimodal transformer model\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer, embedder, transformer, head, hidden_state_index=8):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tokenizer: tokenizer to use\n",
        "            embedder: embedder to use\n",
        "            transformer: transformer to use\n",
        "            head: head to use\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.pos_encoder = PositionalEncoding(EMBEDDING_DIM, dual_modality=False)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embedder = embedder\n",
        "        self.transformer = transformer\n",
        "        self.head = head\n",
        "        self.hidden_state_index = hidden_state_index\n",
        "\n",
        "    def forward(self, texts, pixel_values):\n",
        "        \"\"\"\n",
        "        Forward pass of the model\n",
        "        Args:\n",
        "            texts: texts to use\n",
        "            audio_features: audio features to use\n",
        "            audio_attentions: audio attentions to use\n",
        "        \"\"\"\n",
        "        tokenizer_output = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=False).to(device)\n",
        "        embedder_output = self.embedder(**tokenizer_output, output_hidden_states=True)\n",
        "        if self.hidden_state_index == -1:\n",
        "            text_features = embedder_output['last_hidden_state']\n",
        "        else:\n",
        "            assert self.hidden_state_index < len(embedder_output['hidden_states']), f'hidden_state_index must be between 0 and {len(embedder_output[\"hidden_states\"])}'\n",
        "            text_features = embedder_output['hidden_states'][self.hidden_state_index]\n",
        "        #text_features_sum = (text_features * tokenizer_output.attention_mask.unsqueeze(-1)).sum(axis=1)\n",
        "        #text_features_pooled = text_features_sum / tokenizer_output.attention_mask.sum(axis=1).unsqueeze(-1)\n",
        "        text_attentions = tokenizer_output.attention_mask\n",
        "\n",
        "        pixel_values_0 = torch.tensor(pixel_values)\n",
        "\n",
        "        outputs_att = self.vit(pixel_values_0, output_attentions=True, interpolate_pos_encoding=True)\n",
        "\n",
        "        audio_attentions = outputs_att.attentions\n",
        "\n",
        "        ''''audio_attentions = outputs_att.attentions[-1] # we are only interested in the attention maps of the last layer\n",
        "        nh = audio_attentions.shape[1] # number of head\n",
        "        audio_attentions = audio_attentions[0, :, 0, 1:].reshape(nh, -1)'''\n",
        "\n",
        "        print('audio_attentions[0]',audio_attentions[0].shape, type(audio_attentions), type(audio_attentions[0]), 'len audio_attentions', len(audio_attentions))\n",
        "        print('text_attentions', text_attentions.shape, type(text_attentions))\n",
        "\n",
        "        concatenated_attentions = torch.cat((text_attentions,audio_attentions[0].flatten(1)), dim=1)\n",
        "        for tth in audio_attentions[1:]:\n",
        "          concatenated_attentions = torch.cat((concatenated_attentions, tth.flatten(1)), dim=1)\n",
        "\n",
        "        #concatenated_attentions = torch.cat((text_attentions, torch.tensor(audio_attentions).clon().detech().flatten(1)), dim=1)\n",
        "\n",
        "        audio_features = outputs_att.last_hidden_state\n",
        "\n",
        "        audio_features = self.pos_encoder(audio_features)\n",
        "\n",
        "        print('text_features',text_features.shape, 'audio_features', audio_features.shape)\n",
        "\n",
        "        concatenated_features = torch.cat((text_features, audio_features), dim=1)\n",
        "\n",
        "        print('audio_attentions[0]',audio_attentions[0].shape, type(audio_attentions), type(audio_attentions[0]))\n",
        "        print('text_attentions', text_attentions.shape, type(text_attentions))\n",
        "\n",
        "        transformer_output = self.transformer(concatenated_features, text_attentions, audio_attentions[0].flatten(1))\n",
        "\n",
        "        # pooling of transformer output\n",
        "        transformer_output_sum = (transformer_output * concatenated_attentions.unsqueeze(-1)).sum(axis=1) # 4 d d 768 d\n",
        "        transformer_output_pooled = transformer_output_sum / concatenated_attentions.sum(axis=1).unsqueeze(-1) # 4 d 768 d\n",
        "        return self.head(transformer_output_pooled) #4, 768\n",
        "    def get_output_attentions(self, pixel_values):\n",
        "        return self.vit(torch.tensor(pixel_values), output_attentions=True, interpolate_pos_encoding=True)"
      ],
      "metadata": {
        "id": "25ecdjXkvzzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([[1, 2, 4], [8, 16, 32]]).sum(axis=1)"
      ],
      "metadata": {
        "id": "YqcwfyUCfTfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([[1, 2, 4], [8, 16, 32]]).shape"
      ],
      "metadata": {
        "id": "rIIVEmtMfvMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEEDS = [1, 42, 69, 100, 420]\n",
        "SEEDS = [1]\n",
        "\n",
        "dict_model_names = ['MMT','Image-Only','Text-Only']\n",
        "#dict_model_names = ['Image-Only']\n",
        "#dict_model_names = ['Text-Only']\n",
        "#dict_model_names = ['img_only']\n",
        "\n",
        "# initialize results\n",
        "val_results = {n : [] for n in dict_model_names}\n",
        "test_results = {n : [] for n in dict_model_names}\n",
        "\n",
        "# initialize history\n",
        "history_train_losses = {n : [] for n in dict_model_names}\n",
        "history_train_accuracy = {n : [] for n in dict_model_names}\n",
        "history_train_f1 = {n : [] for n in dict_model_names}\n",
        "history_val_losses = {n : [] for n in dict_model_names}\n",
        "history_val_accuracy = {n : [] for n in dict_model_names}\n",
        "history_val_f1 = {n : [] for n in dict_model_names}\n",
        "\n",
        "EPOCHS = 2\n",
        "INITIAL_LR = 1e-4\n",
        "WEIGHT_DECAY = 1e-3\n",
        "LR_DECAY_FACTOR = 1e-1\n",
        "LR_DECAY_PATIENCE = 3\n",
        "VERBOSE_TRAIN = True\n",
        "DEBUG_TRAIN = False\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(f'{f\"TRAINING WITH SEED {seed}\":=^100}')\n",
        "    print()\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    model_names, models = create_models()\n",
        "\n",
        "    # TO select a single model (for debug purposes):\n",
        "    # model_idx = 4\n",
        "    # models, model_names = [models[model_idx]], [model_names[model_idx]]\n",
        "\n",
        "    while models:\n",
        "        model = models[0]\n",
        "        model_name = model_names[0]\n",
        "\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        print(f'{f\"Training model {model_name}\":_^100}')\n",
        "\n",
        "        loss = nn.CrossEntropyLoss(weight=weight)\n",
        "\n",
        "        if model_name == 'ensembling':\n",
        "            # ensembling model uses a different loss function\n",
        "            loss = lambda outputs, targets: torch.nn.functional.nll_loss(torch.log(outputs), targets, weight=weight, reduction='mean')\n",
        "\n",
        "        _, history = train(\n",
        "            model,\n",
        "            loss,\n",
        "            train_dataloader,\n",
        "            val_dataloader,\n",
        "            epochs=EPOCHS,\n",
        "            device=device,\n",
        "            lr=INITIAL_LR,\n",
        "            lr_decay_factor=LR_DECAY_FACTOR,\n",
        "            lr_decay_patience=LR_DECAY_PATIENCE,\n",
        "            weight_decay=WEIGHT_DECAY,\n",
        "            verbose=VERBOSE_TRAIN,\n",
        "            debug = DEBUG_TRAIN\n",
        "        )\n",
        "\n",
        "        history_train_losses[model_name].append(history['train_loss'])\n",
        "        history_train_accuracy[model_name].append(history['train_accuracy'])\n",
        "        #history_train_f1[model_name].append(history['train_f1'])\n",
        "        history_val_losses[model_name].append(history['val_loss'])\n",
        "        history_val_accuracy[model_name].append(history['val_accuracy'])\n",
        "        #history_val_f1[model_name].append(history['val_f1'])\n",
        "\n",
        "        #_, val_acc, val_f1, val_pred, val_targ, val_logits = evaluate(model, val_dataloader, loss)\n",
        "        #val_results[model_name].append({'acc': val_acc, 'f1': val_f1, 'pred': val_pred, 'targ': val_targ, 'logits':val_logits})\n",
        "        _, val_acc, val_pred, val_targ, val_logits = evaluate(model, val_dataloader, loss)\n",
        "        val_results[model_name].append({'acc': val_acc, 'pred': val_pred, 'targ': val_targ, 'logits':val_logits})\n",
        "\n",
        "        #_, test_acc, test_f1, test_pred, test_targ, test_logits = evaluate(model, test_dataloader, loss)\n",
        "        #test_results[model_name].append({'acc': test_acc, 'f1': test_f1, 'pred': test_pred, 'targ': test_targ, 'logits':test_logits})\n",
        "        _, test_acc, test_pred, test_targ, test_logits = evaluate(model, test_dataloader, loss)\n",
        "        test_results[model_name].append({'acc': test_acc, 'pred': test_pred, 'targ': test_targ, 'logits':test_logits})\n",
        "\n",
        "        if VERBOSE_TRAIN:\n",
        "            #'print(f'[VAL] Model: {model_name} - acc: {val_acc:.4f} - f1: {val_f1:.4f}')\n",
        "            #print(f'[TEST] Model: {model_name} - acc: {test_acc:.4f} - f1: {test_f1:.4f}')\n",
        "\n",
        "            print(f'[VAL] Model: {model_name} - acc: {val_acc:.4f}')\n",
        "            print(f'[TEST] Model: {model_name} - acc: {test_acc:.4f}')\n",
        "            print()\n",
        "\n",
        "        # delete model to free up memory and avoid memory errors\n",
        "        del model\n",
        "        del models[0]\n",
        "        del model_names[0]\n",
        "        gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx-DpjWH_poZ",
        "outputId": "9dd6bfd2-8a68-4dbb-f79d-056c7b319d27"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================TRAINING WITH SEED 1========================================\n",
            "\n",
            "_________________________________________Training model MMT_________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 76]) <class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-82a8b042a707>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  audio_attentions = m(torch.tensor(audio_attentions[0]).flatten(1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "concatenated_attentions torch.Size([4, 273])\n",
            "text_features torch.Size([4, 76, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 273, 768])\n",
            "text_attentions torch.Size([4, 76]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 61]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 258])\n",
            "text_features torch.Size([4, 61, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 258, 768])\n",
            "text_attentions torch.Size([4, 61]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 75]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 272])\n",
            "text_features torch.Size([4, 75, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 272, 768])\n",
            "text_attentions torch.Size([4, 75]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 62]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 259])\n",
            "text_features torch.Size([4, 62, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 259, 768])\n",
            "text_attentions torch.Size([4, 62]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 44]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 241])\n",
            "text_features torch.Size([4, 44, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 241, 768])\n",
            "text_attentions torch.Size([4, 44]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 76]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 273])\n",
            "text_features torch.Size([4, 76, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 273, 768])\n",
            "text_attentions torch.Size([4, 76]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 60]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 257])\n",
            "text_features torch.Size([4, 60, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 257, 768])\n",
            "text_attentions torch.Size([4, 60]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 92]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 289])\n",
            "text_features torch.Size([4, 92, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 289, 768])\n",
            "text_attentions torch.Size([4, 92]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 62]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 259])\n",
            "text_features torch.Size([4, 62, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 259, 768])\n",
            "text_attentions torch.Size([4, 62]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 60]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 257])\n",
            "text_features torch.Size([4, 60, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 257, 768])\n",
            "text_attentions torch.Size([4, 60]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 56]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 253])\n",
            "text_features torch.Size([4, 56, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 253, 768])\n",
            "text_attentions torch.Size([4, 56]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 89]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 286])\n",
            "text_features torch.Size([4, 89, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 286, 768])\n",
            "text_attentions torch.Size([4, 89]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 60]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 257])\n",
            "text_features torch.Size([4, 60, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 257, 768])\n",
            "text_attentions torch.Size([4, 60]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 88]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 285])\n",
            "text_features torch.Size([4, 88, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 285, 768])\n",
            "text_attentions torch.Size([4, 88]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 52]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 249])\n",
            "text_features torch.Size([4, 52, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 249, 768])\n",
            "text_attentions torch.Size([4, 52]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 88]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 285])\n",
            "text_features torch.Size([4, 88, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 285, 768])\n",
            "text_attentions torch.Size([4, 88]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 62]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 259])\n",
            "text_features torch.Size([4, 62, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 259, 768])\n",
            "text_attentions torch.Size([4, 62]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 92]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 289])\n",
            "text_features torch.Size([4, 92, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 289, 768])\n",
            "text_attentions torch.Size([4, 92]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 59]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 256])\n",
            "text_features torch.Size([4, 59, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 256, 768])\n",
            "text_attentions torch.Size([4, 59]) audio_attentions torch.Size([4, 197])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [03:56<03:56, 236.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Training Loss: 0.1788, Validation Loss: 0.1835, accuracy = 0.5000\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 56]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 253])\n",
            "text_features torch.Size([4, 56, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 253, 768])\n",
            "text_attentions torch.Size([4, 56]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 63]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 260])\n",
            "text_features torch.Size([4, 63, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 260, 768])\n",
            "text_attentions torch.Size([4, 63]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 92]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 289])\n",
            "text_features torch.Size([4, 92, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 289, 768])\n",
            "text_attentions torch.Size([4, 92]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 62]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 259])\n",
            "text_features torch.Size([4, 62, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 259, 768])\n",
            "text_attentions torch.Size([4, 62]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 75]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 272])\n",
            "text_features torch.Size([4, 75, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 272, 768])\n",
            "text_attentions torch.Size([4, 75]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 52]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 249])\n",
            "text_features torch.Size([4, 52, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 249, 768])\n",
            "text_attentions torch.Size([4, 52]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 48]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 245])\n",
            "text_features torch.Size([4, 48, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 245, 768])\n",
            "text_attentions torch.Size([4, 48]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 49]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 246])\n",
            "text_features torch.Size([4, 49, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 246, 768])\n",
            "text_attentions torch.Size([4, 49]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 89]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 286])\n",
            "text_features torch.Size([4, 89, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 286, 768])\n",
            "text_attentions torch.Size([4, 89]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 76]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 273])\n",
            "text_features torch.Size([4, 76, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 273, 768])\n",
            "text_attentions torch.Size([4, 76]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 88]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 285])\n",
            "text_features torch.Size([4, 88, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 285, 768])\n",
            "text_attentions torch.Size([4, 88]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 60]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 257])\n",
            "text_features torch.Size([4, 60, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 257, 768])\n",
            "text_attentions torch.Size([4, 60]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 75]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 272])\n",
            "text_features torch.Size([4, 75, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 272, 768])\n",
            "text_attentions torch.Size([4, 75]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 60]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 257])\n",
            "text_features torch.Size([4, 60, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 257, 768])\n",
            "text_attentions torch.Size([4, 60]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 79]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 276])\n",
            "text_features torch.Size([4, 79, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 276, 768])\n",
            "text_attentions torch.Size([4, 79]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 46]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 243])\n",
            "text_features torch.Size([4, 46, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 243, 768])\n",
            "text_attentions torch.Size([4, 46]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 76]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 273])\n",
            "text_features torch.Size([4, 76, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 273, 768])\n",
            "text_attentions torch.Size([4, 76]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 60]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 257])\n",
            "text_features torch.Size([4, 60, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 257, 768])\n",
            "text_attentions torch.Size([4, 60]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 59]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 256])\n",
            "text_features torch.Size([4, 59, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 256, 768])\n",
            "text_attentions torch.Size([4, 59]) audio_attentions torch.Size([4, 197])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [07:47<00:00, 233.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 0.1649, Validation Loss: 0.1773, accuracy = 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 59]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 256])\n",
            "text_features torch.Size([4, 59, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 256, 768])\n",
            "text_attentions torch.Size([4, 59]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 79]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 276])\n",
            "text_features torch.Size([4, 79, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 276, 768])\n",
            "text_attentions torch.Size([4, 79]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 62]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 259])\n",
            "text_features torch.Size([4, 62, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 259, 768])\n",
            "text_attentions torch.Size([4, 62]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([4, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([4, 63]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([4, 260])\n",
            "text_features torch.Size([4, 63, 768]) audio_features torch.Size([4, 197, 768])\n",
            "concatenated_features torch.Size([4, 260, 768])\n",
            "text_attentions torch.Size([4, 63]) audio_attentions torch.Size([4, 197])\n",
            "audio_attentions[0] torch.Size([2, 12, 197, 197]) <class 'tuple'> <class 'torch.Tensor'> len audio_attentions 12\n",
            "text_attentions torch.Size([2, 89]) <class 'torch.Tensor'>\n",
            "concatenated_attentions torch.Size([2, 286])\n",
            "text_features torch.Size([2, 89, 768]) audio_features torch.Size([2, 197, 768])\n",
            "concatenated_features torch.Size([2, 286, 768])\n",
            "text_attentions torch.Size([2, 89]) audio_attentions torch.Size([2, 197])\n",
            "[VAL] Model: MMT - acc: 0.5000\n",
            "[TEST] Model: MMT - acc: 0.7143\n",
            "\n",
            "_____________________________________Training model Image-Only______________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [03:42<03:42, 222.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "Epoch: 0, Training Loss: 0.1775, Validation Loss: 0.1977, accuracy = 0.5000\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [07:25<00:00, 222.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "Epoch: 1, Training Loss: 0.1749, Validation Loss: 0.1941, accuracy = 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([4, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([4, 768])\n",
            "last_hidden_states_0 torch.Size([4, 768])\n",
            "output torch.Size([4, 768])\n",
            "pixel_values_0 torch.Size([2, 3, 224, 224])\n",
            "type outputs <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
            "attentions 12\n",
            "last_hidden_states torch.Size([2, 768])\n",
            "last_hidden_states_0 torch.Size([2, 768])\n",
            "output torch.Size([2, 768])\n",
            "[VAL] Model: Image-Only - acc: 0.5000\n",
            "[TEST] Model: Image-Only - acc: 0.7143\n",
            "\n",
            "______________________________________Training model Text-Only______________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_features torch.Size([4, 76, 768])\n",
            "text_attentions torch.Size([4, 76])\n",
            "text_features torch.Size([4, 76, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 61, 768])\n",
            "text_attentions torch.Size([4, 61])\n",
            "text_features torch.Size([4, 61, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 75, 768])\n",
            "text_attentions torch.Size([4, 75])\n",
            "text_features torch.Size([4, 75, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 62, 768])\n",
            "text_attentions torch.Size([4, 62])\n",
            "text_features torch.Size([4, 62, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 44, 768])\n",
            "text_attentions torch.Size([4, 44])\n",
            "text_features torch.Size([4, 44, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 76, 768])\n",
            "text_attentions torch.Size([4, 76])\n",
            "text_features torch.Size([4, 76, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 60, 768])\n",
            "text_attentions torch.Size([4, 60])\n",
            "text_features torch.Size([4, 60, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 92, 768])\n",
            "text_attentions torch.Size([4, 92])\n",
            "text_features torch.Size([4, 92, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 62, 768])\n",
            "text_attentions torch.Size([4, 62])\n",
            "text_features torch.Size([4, 62, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 60, 768])\n",
            "text_attentions torch.Size([4, 60])\n",
            "text_features torch.Size([4, 60, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 56, 768])\n",
            "text_attentions torch.Size([4, 56])\n",
            "text_features torch.Size([4, 56, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 89, 768])\n",
            "text_attentions torch.Size([4, 89])\n",
            "text_features torch.Size([4, 89, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 60, 768])\n",
            "text_attentions torch.Size([4, 60])\n",
            "text_features torch.Size([4, 60, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 88, 768])\n",
            "text_attentions torch.Size([4, 88])\n",
            "text_features torch.Size([4, 88, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 52, 768])\n",
            "text_attentions torch.Size([4, 52])\n",
            "text_features torch.Size([4, 52, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 88, 768])\n",
            "text_attentions torch.Size([4, 88])\n",
            "text_features torch.Size([4, 88, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 62, 768])\n",
            "text_attentions torch.Size([4, 62])\n",
            "text_features torch.Size([4, 62, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 92, 768])\n",
            "text_attentions torch.Size([4, 92])\n",
            "text_features torch.Size([4, 92, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:25<00:25, 25.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_features torch.Size([4, 59, 768])\n",
            "text_attentions torch.Size([4, 59])\n",
            "text_features torch.Size([4, 59, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "Epoch: 0, Training Loss: 0.1737, Validation Loss: 0.1929, accuracy = 0.5000\n",
            "text_features torch.Size([4, 56, 768])\n",
            "text_attentions torch.Size([4, 56])\n",
            "text_features torch.Size([4, 56, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 75, 768])\n",
            "text_attentions torch.Size([4, 75])\n",
            "text_features torch.Size([4, 75, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 88, 768])\n",
            "text_attentions torch.Size([4, 88])\n",
            "text_features torch.Size([4, 88, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 63, 768])\n",
            "text_attentions torch.Size([4, 63])\n",
            "text_features torch.Size([4, 63, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 60, 768])\n",
            "text_attentions torch.Size([4, 60])\n",
            "text_features torch.Size([4, 60, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 92, 768])\n",
            "text_attentions torch.Size([4, 92])\n",
            "text_features torch.Size([4, 92, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 61, 768])\n",
            "text_attentions torch.Size([4, 61])\n",
            "text_features torch.Size([4, 61, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 76, 768])\n",
            "text_attentions torch.Size([4, 76])\n",
            "text_features torch.Size([4, 76, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 79, 768])\n",
            "text_attentions torch.Size([4, 79])\n",
            "text_features torch.Size([4, 79, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 60, 768])\n",
            "text_attentions torch.Size([4, 60])\n",
            "text_features torch.Size([4, 60, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 47, 768])\n",
            "text_attentions torch.Size([4, 47])\n",
            "text_features torch.Size([4, 47, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 62, 768])\n",
            "text_attentions torch.Size([4, 62])\n",
            "text_features torch.Size([4, 62, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 62, 768])\n",
            "text_attentions torch.Size([4, 62])\n",
            "text_features torch.Size([4, 62, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 52, 768])\n",
            "text_attentions torch.Size([4, 52])\n",
            "text_features torch.Size([4, 52, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 52, 768])\n",
            "text_attentions torch.Size([4, 52])\n",
            "text_features torch.Size([4, 52, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 92, 768])\n",
            "text_attentions torch.Size([4, 92])\n",
            "text_features torch.Size([4, 92, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 60, 768])\n",
            "text_attentions torch.Size([4, 60])\n",
            "text_features torch.Size([4, 60, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 56, 768])\n",
            "text_attentions torch.Size([4, 56])\n",
            "text_features torch.Size([4, 56, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 25.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_features torch.Size([4, 59, 768])\n",
            "text_attentions torch.Size([4, 59])\n",
            "text_features torch.Size([4, 59, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "Epoch: 1, Training Loss: 0.1702, Validation Loss: 0.1686, accuracy = 0.7500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_features torch.Size([4, 59, 768])\n",
            "text_attentions torch.Size([4, 59])\n",
            "text_features torch.Size([4, 59, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 79, 768])\n",
            "text_attentions torch.Size([4, 79])\n",
            "text_features torch.Size([4, 79, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 62, 768])\n",
            "text_attentions torch.Size([4, 62])\n",
            "text_features torch.Size([4, 62, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([4, 63, 768])\n",
            "text_attentions torch.Size([4, 63])\n",
            "text_features torch.Size([4, 63, 768])\n",
            "text_features_pooled torch.Size([4, 768])\n",
            "text_features torch.Size([2, 89, 768])\n",
            "text_attentions torch.Size([2, 89])\n",
            "text_features torch.Size([2, 89, 768])\n",
            "text_features_pooled torch.Size([2, 768])\n",
            "[VAL] Model: Text-Only - acc: 0.7500\n",
            "[TEST] Model: Text-Only - acc: 0.9286\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Fef2cnDiCyiB"
      },
      "outputs": [],
      "source": [
        "def create_models(\n",
        "        head_hidden_dimension=224, dropout_prob=0.1, hidden_state_index=8,   # shared parameters head_hidden_dimension=256\n",
        "        audioonly_nheads=8, audioonly_d_ffn=100, audioonly_n_layers=1, # audio only parameters\n",
        "        csa_nheads=4, csa_d_ffn=2048, csa_n_layers=1, # multimodal parameters\n",
        "        ensembling_nheads=4, ensembling_d_ffn=2048, ensembling_n_layers=1, # ensembling parameters\n",
        "        multa_nblocks=4, multa_d_ffn=2048 # unaligned parameters\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Helper function to create and return all the models\n",
        "    \"\"\"\n",
        "    ###################################################################################### -- TEXT MODEL --\n",
        "    text_only_head = nn.Sequential(\n",
        "        nn.Linear(EMBEDDING_DIM, head_hidden_dimension),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(head_hidden_dimension, MODEL_NUM_LABELS)\n",
        "    ).to(device)\n",
        "    text_only = TextModel(tokenizer, embedder, text_only_head)\n",
        "\n",
        "    ###################################################################################### -- IMAGE MODEL --\n",
        "    img_only_head = nn.Sequential(\n",
        "        nn.Linear(EMBEDDING_DIM, head_hidden_dimension),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(head_hidden_dimension, MODEL_NUM_LABELS)\n",
        "    ).to(device)\n",
        "    img_only = ViTForImageClassification(img_only_head)\n",
        "    #image_only = models.get('vit_base', num_classes = 2, pretrained_weights='imagenet')\n",
        "    \"\"\"image_size_only_head = nn.Sequential(\n",
        "        nn.Linear(EMBEDDING_DIM, head_hidden_dimension),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(head_hidden_dimension, MODEL_NUM_LABELS)\n",
        "    ).to(device)\n",
        "    image_only = TextModel(tokenizer, embedder, image_only_head)\"\"\"\n",
        "\n",
        "    ###################################################################################### -- MULTIMODAL MODEL --\n",
        "    multimodal_encoder = CustomEncoder(d_model=EMBEDDING_DIM, ffn_hidden=csa_d_ffn, n_head=csa_nheads, n_layers=csa_n_layers, drop_prob=dropout_prob)\n",
        "    multimodal_transformer_head = nn.Sequential(\n",
        "        nn.Linear(EMBEDDING_DIM, head_hidden_dimension),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(head_hidden_dimension, MODEL_NUM_LABELS)\n",
        "    ).to(device)\n",
        "    multimodal_transformer = CSA(tokenizer, embedder, multimodal_encoder, multimodal_transformer_head, hidden_state_index=hidden_state_index).to(device)\n",
        "\n",
        "\n",
        "    ###################################################################################### -- ENSEMBLING MODEL --\n",
        "    \"\"\"ensembling_text_head = nn.Sequential(\n",
        "        nn.Linear(EMBEDDING_DIM, head_hidden_dimension),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(head_hidden_dimension, MODEL_NUM_LABELS)\n",
        "    ).to(device)\n",
        "    ensembling_audio_head = nn.Sequential(\n",
        "        nn.Linear(EMBEDDING_DIM, head_hidden_dimension),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(head_hidden_dimension, MODEL_NUM_LABELS)\n",
        "    ).to(device)\n",
        "    ensembling_transformer_layer = nn.TransformerEncoderLayer(d_model=EMBEDDING_DIM, nhead=ensembling_nheads, dim_feedforward=ensembling_d_ffn, batch_first=True).to(device)\n",
        "    ensembling_transformer_encoder = nn.TransformerEncoder(ensembling_transformer_layer, num_layers=ensembling_n_layers).to(device)\n",
        "    ensembling_text_model = TextModel(tokenizer, embedder, ensembling_text_head)\n",
        "    ensembling_audio_model = AudioModel(ensembling_transformer_encoder, ensembling_audio_head)\n",
        "    ensembling_fusion = Ensembling(ensembling_text_model, ensembling_audio_model).to(device)\"\"\"\n",
        "\n",
        "\n",
        "    ###################################################################################### -- UNALIGNED MODEL --\n",
        "    \"\"\"unaligned_head = nn.Sequential(\n",
        "        nn.Linear(EMBEDDING_DIM*2, head_hidden_dimension),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(head_hidden_dimension, MODEL_NUM_LABELS)\n",
        "    ).to(device)\n",
        "    unaligned_mm_model = MulTA(embedding_dim=EMBEDDING_DIM, d_ffn=multa_d_ffn, n_blocks=multa_nblocks, head=unaligned_head, hidden_state_index=hidden_state_index, dropout_prob=dropout_prob).to(device)\n",
        "    \"\"\"\n",
        "\n",
        "    ###################################################################################### -- RETURN --\n",
        "    model_names = ['MMT','Image-Only', 'Text-Only']\n",
        "    models = [multimodal_transformer, img_only, text_only]\n",
        "    ''''model_names = ['Image-Only']\n",
        "    models = [img_only]'''\n",
        "    \"\"\"model_names = ['Text-Only']\n",
        "    models = [text_only]\"\"\"\n",
        "    \"\"\"model_names = ['img_only']\n",
        "    models = [img_only]\"\"\"\n",
        "    return model_names, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qvHuloEGC2Tp",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "64675f81-5a4a-4fd3-fdb7-d452f2a5f496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'class MM_Dataset(torch.utils.data.Dataset):\\n    \"\"\"\\n    Dataset class for multimodal dataset\\n    \"\"\"\\n    def __init__(self, df, audio_dir, sample_rate):\\n        \"\"\"\\n        Args:\\n            df: dataframe containing the dataset\\n            audio_dir: directory containing the audio clips\\n            sample_rate: sample rate to use for audio clips\\n        \"\"\"\\n        self.audio_dir = audio_dir\\n        self.sample_rate = sample_rate\\n\\n        self.audio_processor = AutoProcessor.from_pretrained(AUDIO_MODEL_CARD)\\n        self.audio_model = AutoModel.from_pretrained(AUDIO_MODEL_CARD).to(device)\\n\\n        self.dataset = []\\n\\n        # Iterate over df\\n        for _, row in tqdm(df.iterrows()):\\n            path = os.path.join(self.audio_dir, f\"{row[\\'Document\\']}/{row[\\'idClip\\']}.wav\")\\n            if os.path.exists(path):\\n                # obtain audio WAV2VEC features\\n                audio, sampling_rate = torchaudio.load(path)\\n                # resample audio if necessary\\n                if sampling_rate != self.sample_rate:\\n                    audio = torchaudio.functional.resample(audio, sample_rate, self.sample_rate)\\n                    # mean pooling over channels\\n                    audio = torch.mean(audio, dim=0, keepdim=True)\\n                with torch.inference_mode():\\n                    # run audio through model\\n                    input_values = self.audio_processor(audio, sampling_rate=self.sample_rate).input_values[0]\\n                    input_values = torch.tensor(input_values).to(device)\\n                    audio_model_output = self.audio_model(input_values)\\n                    audio_features = audio_model_output.last_hidden_state[0].unsqueeze(0)\\n                    # downsample audio features\\n                    audio_features = torch.nn.functional.interpolate(audio_features.permute(0,2,1), scale_factor=DOWNSAMPLE_FACTOR, mode=\\'linear\\')\\n                    audio_features = audio_features.permute(0,2,1)[0]\\n                    audio_features = audio_features.cpu()\\n\\n                text = row[\\'Text\\']\\n\\n                self.dataset.append((text, audio_features, LABEL_2_ID[row[\\'Component\\']]))\\n    def __len__(self):\\n        return len(self.dataset)\\n\\n    def __getitem__(self, index):\\n        return self.dataset[index]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# set up tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(TEXT_MODEL_CARD)\n",
        "embedder = BertModel.from_pretrained(TEXT_MODEL_CARD).to(device)\n",
        "\n",
        "# freeze bert layers\n",
        "for params in embedder.parameters():\n",
        "    params.requires_grad = False\n",
        "\n",
        "# Downsample audio features to 1/5 of the original size to fit in memory\n",
        "DOWNSAMPLE_FACTOR = 1/5\n",
        "\n",
        "'''class MM_Dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for multimodal dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, df, audio_dir, sample_rate):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df: dataframe containing the dataset\n",
        "            audio_dir: directory containing the audio clips\n",
        "            sample_rate: sample rate to use for audio clips\n",
        "        \"\"\"\n",
        "        self.audio_dir = audio_dir\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "        self.audio_processor = AutoProcessor.from_pretrained(AUDIO_MODEL_CARD)\n",
        "        self.audio_model = AutoModel.from_pretrained(AUDIO_MODEL_CARD).to(device)\n",
        "\n",
        "        self.dataset = []\n",
        "\n",
        "        # Iterate over df\n",
        "        for _, row in tqdm(df.iterrows()):\n",
        "            path = os.path.join(self.audio_dir, f\"{row['Document']}/{row['idClip']}.wav\")\n",
        "            if os.path.exists(path):\n",
        "                # obtain audio WAV2VEC features\n",
        "                audio, sampling_rate = torchaudio.load(path)\n",
        "                # resample audio if necessary\n",
        "                if sampling_rate != self.sample_rate:\n",
        "                    audio = torchaudio.functional.resample(audio, sample_rate, self.sample_rate)\n",
        "                    # mean pooling over channels\n",
        "                    audio = torch.mean(audio, dim=0, keepdim=True)\n",
        "                with torch.inference_mode():\n",
        "                    # run audio through model\n",
        "                    input_values = self.audio_processor(audio, sampling_rate=self.sample_rate).input_values[0]\n",
        "                    input_values = torch.tensor(input_values).to(device)\n",
        "                    audio_model_output = self.audio_model(input_values)\n",
        "                    audio_features = audio_model_output.last_hidden_state[0].unsqueeze(0)\n",
        "                    # downsample audio features\n",
        "                    audio_features = torch.nn.functional.interpolate(audio_features.permute(0,2,1), scale_factor=DOWNSAMPLE_FACTOR, mode='linear')\n",
        "                    audio_features = audio_features.permute(0,2,1)[0]\n",
        "                    audio_features = audio_features.cpu()\n",
        "\n",
        "                text = row['Text']\n",
        "\n",
        "                self.dataset.append((text, audio_features, LABEL_2_ID[row['Component']]))\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.dataset[index]'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9vW6Vs80C4e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82949a94-b734-40a1-b945-961e17454ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Normal: 37: 74.00%\n",
            "Total Lung: 13: 26.00%\n"
          ]
        }
      ],
      "source": [
        "num_claim = len(df1.loc[df1['Problems'].isin(['normal'])])\n",
        "print(f'Total Normal: {num_claim}: {num_claim*100/len(df1):.2f}%')\n",
        "\n",
        "num_premise = len(df1.loc[df1['Problems'].isin(['Lung'])])\n",
        "print(f'Total Lung: {num_premise}: {num_premise*100/len(df1):.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "uCoK7KUnC6rR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40624ae-fd2c-4162-dd48-3856217e02fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight for loss function: tensor([0.6757, 1.9231])\n"
          ]
        }
      ],
      "source": [
        "if MODEL_NUM_LABELS == 2:\n",
        "    claim_ratio = num_claim / (num_claim + num_premise)\n",
        "    premise_ratio = num_premise / (num_claim + num_premise)\n",
        "    weight = torch.tensor([1/(2*claim_ratio), 1/(2*premise_ratio)]).to(device)\n",
        "else:\n",
        "    claim_ratio = num_claim / (num_claim + num_premise + num_other)\n",
        "    premise_ratio = num_premise / (num_claim + num_premise + num_other)\n",
        "    other_ratio = num_other / (num_claim + num_premise + num_other)\n",
        "    weight = torch.tensor([1/(3*claim_ratio), 1/(3*premise_ratio), 1/(3*other_ratio)]).to(device)\n",
        "\n",
        "print(f'Weight for loss function: {weight}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "crupvMAQC8Pz"
      },
      "outputs": [],
      "source": [
        "class BestModel:\n",
        "    \"\"\"\n",
        "    Class to keep track of the best performing model on validation set during training\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.best_validation_loss = float('Infinity')\n",
        "        self.best_state_dict = None\n",
        "    def __call__(self, model, loss):\n",
        "        if loss < self.best_validation_loss:\n",
        "            self.best_validation_loss = loss\n",
        "            self.best_state_dict = copy.deepcopy(model.state_dict())\n",
        "\n",
        "def evaluate(model, data_loader, loss_fn, debug=False):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the set passed\n",
        "    Args:\n",
        "        model: model to evaluate\n",
        "        data_loader: DataLoader object\n",
        "        loss_fn: loss function to use\n",
        "        debug: whether to print debug statements\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_correct = 0\n",
        "    num_examples = 0\n",
        "    tot_pred, tot_targ, tot_logits = torch.LongTensor(), torch.LongTensor(), torch.LongTensor()\n",
        "    for batch in data_loader:\n",
        "        #texts, img_features, img_attention, targets = batch\n",
        "        texts, img_features, targets = batch\n",
        "        #img_features = img_features.to(device)\n",
        "        #img_attention = img_attention.to(device)\n",
        "        #targets = targets.to(device)\n",
        "        #output = model(texts,img_features,img_attention)\n",
        "        output = model(texts,img_features)\n",
        "        if debug:\n",
        "            print(\"OUTPUT\",output)\n",
        "            print(\"TARGETS\", targets)\n",
        "        loss = loss_fn(output, targets)\n",
        "        total_loss += loss.detach()\n",
        "\n",
        "        # if label O is still in the dataset we remove it from the outputs\n",
        "        # since it's a binary task\n",
        "        if not REMOVE_OTHER:\n",
        "            not_other = targets != 2\n",
        "            output = output[not_other]\n",
        "            targets = targets[not_other]\n",
        "\n",
        "        scores = output[:, :2]\n",
        "        predicted_labels = torch.argmax(scores, dim=-1)\n",
        "\n",
        "        tot_pred = torch.cat((tot_pred, predicted_labels.detach().cpu()))\n",
        "        tot_targ = torch.cat((tot_targ, targets.detach().cpu()))\n",
        "        tot_logits = torch.cat((tot_logits, torch.nn.functional.softmax(scores, dim=-1)[:, 1].detach().cpu()))\n",
        "\n",
        "        correct = torch.eq(predicted_labels, targets).view(-1)\n",
        "        num_correct += torch.sum(correct).item()\n",
        "        num_examples += correct.shape[0]\n",
        "    total_loss = total_loss.cpu().item()\n",
        "    total_loss /= len(data_loader.dataset)\n",
        "    accuracy = num_correct/num_examples\n",
        "    #f1 = multiclass_f1_score(tot_pred, tot_targ, num_classes=2, average=\"macro\")\n",
        "    #return total_loss, accuracy, f1, tot_pred, tot_targ, tot_logits\n",
        "    return total_loss, accuracy, tot_pred, tot_targ, tot_logits\n",
        "\n",
        "\n",
        "def train(model, loss_fn, train_loader, val_loader, epochs=10, device=\"cuda\", lr=1e-3, lr_decay_factor=0.1, lr_decay_patience=3, weight_decay=1e-5, verbose=True, debug=False):\n",
        "    \"\"\"\n",
        "    Train the model on the train set and evaluate on the validation set with the given parameters\n",
        "    Args:\n",
        "        model: model to train\n",
        "        loss_fn: loss function to use\n",
        "        train_loader: DataLoader object for train set\n",
        "        val_loader: DataLoader object for validation set\n",
        "        epochs: number of epochs\n",
        "        device: device to use\n",
        "        lr: initial learning rate\n",
        "        lr_decay_factor: factor to decay learning rate\n",
        "        lr_decay_patience: patience for learning rate decay\n",
        "        weight_decay: weight decay\n",
        "        verbose: whether to print training results\n",
        "        debug: whether to print debug statements\n",
        "    \"\"\"\n",
        "    # set up optimizer and scheduler\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_decay_factor, patience=lr_decay_patience, verbose=True)\n",
        "    best_model_tracker = BestModel()\n",
        "    # history of train and validation losses, accuracy and f1\n",
        "    history_train_losses = []\n",
        "    history_train_accuracy = []\n",
        "    #history_train_f1 = []\n",
        "\n",
        "    history_val_losses = []\n",
        "    history_val_accuracy = []\n",
        "    #history_val_f1 = []\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # training\n",
        "        correct = 0\n",
        "        training_loss = 0.0\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            #texts, img_features, img_attention, targets = batch\n",
        "            texts, img_features, targets = batch\n",
        "            #print('img_features',img_features)\n",
        "            #print('targets',targets)\n",
        "            #img_features = img_features.to(device)\n",
        "            #img_attention = img_attention.to(device)\n",
        "            #targets = targets.to(device)\n",
        "            #output = model(texts,img_features,img_attention)\n",
        "            output = model(texts,img_features)\n",
        "            loss = loss_fn(output, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            correct += torch.eq(torch.argmax(output, dim=-1), targets).view(-1).sum()\n",
        "            training_loss += loss.detach()\n",
        "        training_loss = training_loss.cpu().item()\n",
        "\n",
        "        training_loss /= len(train_loader.dataset)\n",
        "        training_accuracy = correct.item() / len(train_loader.dataset)\n",
        "        #training_f1 = multiclass_f1_score(torch.argmax(output, dim=-1), targets, num_classes=2, average=\"macro\")\n",
        "\n",
        "        #valid_loss, valid_accuracy, valid_f1, _, _, _ = evaluate(model, val_loader, loss_fn, debug)\n",
        "        valid_loss, valid_accuracy, _, _, _ = evaluate(model, val_loader, loss_fn, debug)\n",
        "\n",
        "        history_train_losses.append(training_loss)\n",
        "        history_train_accuracy.append(training_accuracy)\n",
        "        #history_train_f1.append(training_f1)\n",
        "\n",
        "        history_val_losses.append(valid_loss)\n",
        "        history_val_accuracy.append(valid_accuracy)\n",
        "        #history_val_f1.append(valid_f1)\n",
        "\n",
        "        best_model_tracker(model, valid_loss)\n",
        "        scheduler.step(valid_loss)\n",
        "        if verbose:\n",
        "            #print(f'Epoch: {epoch}, Training Loss: {training_loss:.4f}, Validation Loss: {valid_loss:.4f}, accuracy = {valid_accuracy:.4f}, F1={valid_f1:.4f}')\n",
        "            print(f'Epoch: {epoch}, Training Loss: {training_loss:.4f}, Validation Loss: {valid_loss:.4f}, accuracy = {valid_accuracy:.4f}')\n",
        "    # restore best model weights\n",
        "    model.load_state_dict(best_model_tracker.best_state_dict)\n",
        "    history = {\n",
        "        'train_loss': history_train_losses,\n",
        "        'train_accuracy': history_train_accuracy,\n",
        "        'train_f1': history_train_f1,\n",
        "        'val_loss': history_val_losses,\n",
        "        'val_accuracy': history_val_accuracy\n",
        "        #'val_f1': history_val_f1\n",
        "    }\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "d4ShSoN_2KBK"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oav0BkXIABq7"
      },
      "outputs": [],
      "source": [
        "SEEDS = [1, 42, 69, 100, 420]\n",
        "SEEDS = [1]\n",
        "\n",
        "dict_model_names = ['MMT','Image-Only','Text-Only']\n",
        "#dict_model_names = ['Text-Only']\n",
        "#dict_model_names = ['img_only']\n",
        "\n",
        "# initialize results\n",
        "val_results = {n : [] for n in dict_model_names}\n",
        "test_results = {n : [] for n in dict_model_names}\n",
        "\n",
        "# initialize history\n",
        "history_train_losses = {n : [] for n in dict_model_names}\n",
        "history_train_accuracy = {n : [] for n in dict_model_names}\n",
        "history_train_f1 = {n : [] for n in dict_model_names}\n",
        "history_val_losses = {n : [] for n in dict_model_names}\n",
        "history_val_accuracy = {n : [] for n in dict_model_names}\n",
        "history_val_f1 = {n : [] for n in dict_model_names}\n",
        "\n",
        "EPOCHS = 60\n",
        "INITIAL_LR = 1e-4\n",
        "WEIGHT_DECAY = 1e-3\n",
        "LR_DECAY_FACTOR = 1e-1\n",
        "LR_DECAY_PATIENCE = 3\n",
        "VERBOSE_TRAIN = True\n",
        "DEBUG_TRAIN = False\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(f'{f\"TRAINING WITH SEED {seed}\":=^100}')\n",
        "    print()\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    model_names, models = create_models()\n",
        "\n",
        "    # TO select a single model (for debug purposes):\n",
        "    # model_idx = 4\n",
        "    # models, model_names = [models[model_idx]], [model_names[model_idx]]\n",
        "\n",
        "    while models:\n",
        "        model = models[0]\n",
        "        model_name = model_names[0]\n",
        "\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        print(f'{f\"Training model {model_name}\":_^100}')\n",
        "\n",
        "        loss = nn.CrossEntropyLoss(weight=weight)\n",
        "\n",
        "        if model_name == 'ensembling':\n",
        "            # ensembling model uses a different loss function\n",
        "            loss = lambda outputs, targets: torch.nn.functional.nll_loss(torch.log(outputs), targets, weight=weight, reduction='mean')\n",
        "\n",
        "        _, history = train(\n",
        "            model,\n",
        "            loss,\n",
        "            train_dataloader,\n",
        "            val_dataloader,\n",
        "            epochs=EPOCHS,\n",
        "            device=device,\n",
        "            lr=INITIAL_LR,\n",
        "            lr_decay_factor=LR_DECAY_FACTOR,\n",
        "            lr_decay_patience=LR_DECAY_PATIENCE,\n",
        "            weight_decay=WEIGHT_DECAY,\n",
        "            verbose=VERBOSE_TRAIN,\n",
        "            debug = DEBUG_TRAIN\n",
        "        )\n",
        "\n",
        "        history_train_losses[model_name].append(history['train_loss'])\n",
        "        history_train_accuracy[model_name].append(history['train_accuracy'])\n",
        "        #history_train_f1[model_name].append(history['train_f1'])\n",
        "        history_val_losses[model_name].append(history['val_loss'])\n",
        "        history_val_accuracy[model_name].append(history['val_accuracy'])\n",
        "        #history_val_f1[model_name].append(history['val_f1'])\n",
        "\n",
        "        #_, val_acc, val_f1, val_pred, val_targ, val_logits = evaluate(model, val_dataloader, loss)\n",
        "        #val_results[model_name].append({'acc': val_acc, 'f1': val_f1, 'pred': val_pred, 'targ': val_targ, 'logits':val_logits})\n",
        "        _, val_acc, val_pred, val_targ, val_logits = evaluate(model, val_dataloader, loss)\n",
        "        val_results[model_name].append({'acc': val_acc, 'pred': val_pred, 'targ': val_targ, 'logits':val_logits})\n",
        "\n",
        "        #_, test_acc, test_f1, test_pred, test_targ, test_logits = evaluate(model, test_dataloader, loss)\n",
        "        #test_results[model_name].append({'acc': test_acc, 'f1': test_f1, 'pred': test_pred, 'targ': test_targ, 'logits':test_logits})\n",
        "        _, test_acc, test_pred, test_targ, test_logits = evaluate(model, test_dataloader, loss)\n",
        "        test_results[model_name].append({'acc': test_acc, 'pred': test_pred, 'targ': test_targ, 'logits':test_logits})\n",
        "\n",
        "        if VERBOSE_TRAIN:\n",
        "            #'print(f'[VAL] Model: {model_name} - acc: {val_acc:.4f} - f1: {val_f1:.4f}')\n",
        "            #print(f'[TEST] Model: {model_name} - acc: {test_acc:.4f} - f1: {test_f1:.4f}')\n",
        "\n",
        "            print(f'[VAL] Model: {model_name} - acc: {val_acc:.4f}')\n",
        "            print(f'[TEST] Model: {model_name} - acc: {test_acc:.4f}')\n",
        "            print()\n",
        "\n",
        "        # delete model to free up memory and avoid memory errors\n",
        "        del model\n",
        "        del models[0]\n",
        "        del model_names[0]\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMQ41sZD_Abg"
      },
      "outputs": [],
      "source": [
        "models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olmHE5uI72XW"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp0lYlDX7PsB"
      },
      "outputs": [],
      "source": [
        "# save history\n",
        "save_path = '/content/drive/MyDrive/M Ð”Ð¸Ð¿Ð»Ð¾Ð¼/MM history'\n",
        "with open(f'{save_path}/history_train_losses.pkl', 'wb') as f:\n",
        "    pickle.dump(history_train_losses, f)\n",
        "with open(f'{save_path}/history_train_accuracy.pkl', 'wb') as f:\n",
        "    pickle.dump(history_train_accuracy, f)\n",
        "with open(f'{save_path}/history_train_f1.pkl', 'wb') as f:\n",
        "    pickle.dump(history_train_f1, f)\n",
        "\n",
        "with open(f'{save_path}/history_val_losses.pkl', 'wb') as f:\n",
        "    pickle.dump(history_val_losses, f)\n",
        "with open(f'{save_path}/history_val_accuracy.pkl', 'wb') as f:\n",
        "    pickle.dump(history_val_accuracy, f)\n",
        "with open(f'{save_path}/history_val_f1.pkl', 'wb') as f:\n",
        "    pickle.dump(history_val_f1, f)\n",
        "\n",
        "# save results\n",
        "with open(f'{save_path}/val_results.pkl', 'wb') as f:\n",
        "    pickle.dump(val_results, f)\n",
        "with open(f'{save_path}/test_results.pkl', 'wb') as f:\n",
        "    pickle.dump(test_results, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNONFr6CBGvl",
        "papermill": {
          "duration": 0.140558,
          "end_time": "2023-08-30T20:43:51.408596",
          "exception": false,
          "start_time": "2023-08-30T20:43:51.268038",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Image Encoder | ViT Model (Vision Transformer Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:25:24.631889Z",
          "iopub.status.busy": "2024-01-28T06:25:24.631172Z",
          "iopub.status.idle": "2024-01-28T06:25:25.744364Z",
          "shell.execute_reply": "2024-01-28T06:25:25.743515Z",
          "shell.execute_reply.started": "2024-01-28T06:25:24.631853Z"
        },
        "id": "x_bKSY8MBGvl"
      },
      "outputs": [],
      "source": [
        "image_path = 'E:/LangsImgDatasetForDiploma/images_normalized/1000_IM-0003-1001.dcm.png'\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Open the image\n",
        "\n",
        "original_image = Image.open(image_path)\n",
        "\n",
        "# Convert to RGB mode\n",
        "original_image = original_image.convert('RGB')\n",
        "\n",
        "# Resize the image to a new width and height\n",
        "new_width = 300  # Specify your desired width\n",
        "new_height = 200  # Specify your desired height\n",
        "resized_image = original_image.resize((new_width, new_height))\n",
        "\n",
        "# Display the original and resized images\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(original_image)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Resized Image')\n",
        "plt.imshow(resized_image)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SgkP9DMBGvl",
        "papermill": {
          "duration": 0.149266,
          "end_time": "2023-08-30T20:43:51.973351",
          "exception": false,
          "start_time": "2023-08-30T20:43:51.824085",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:37:29.8263Z",
          "iopub.status.busy": "2024-01-28T06:37:29.825945Z",
          "iopub.status.idle": "2024-01-28T06:37:30.051333Z",
          "shell.execute_reply": "2024-01-28T06:37:30.043541Z",
          "shell.execute_reply.started": "2024-01-28T06:37:29.826273Z"
        },
        "id": "b4lD7qiyBGvl",
        "papermill": {
          "duration": 0.364379,
          "end_time": "2023-08-30T20:43:52.47632",
          "exception": false,
          "start_time": "2023-08-30T20:43:52.111941",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "image = image_preprocessing(\"E:/LangsImgDatasetForDiploma/images_normalized/1000_IM-0003-1001.dcm.png\",image_size)\n",
        "print(image.shape)\n",
        "display_image(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:37:50.128225Z",
          "iopub.status.busy": "2024-01-28T06:37:50.127324Z",
          "iopub.status.idle": "2024-01-28T06:38:06.033745Z",
          "shell.execute_reply": "2024-01-28T06:38:06.032821Z",
          "shell.execute_reply.started": "2024-01-28T06:37:50.128191Z"
        },
        "id": "UqfQIZJVBGvm",
        "papermill": {
          "duration": 11.831255,
          "end_time": "2023-08-30T20:44:04.453615",
          "exception": false,
          "start_time": "2023-08-30T20:43:52.62236",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_image_patches(image, patch_size):\n",
        "    batch_size, channels, height, width = image.size()\n",
        "    patches = []\n",
        "    for h in range(0, height, patch_size):\n",
        "        for w in range(0, width, patch_size):\n",
        "            patch = image[:, :, h:h+patch_size, w:w+patch_size]\n",
        "            patches.append(patch)\n",
        "    patches = torch.cat(patches, dim=0)\n",
        "    return patches\n",
        "\n",
        "def plot_patches(patches):\n",
        "    num_patches = patches.size(0)\n",
        "    n = int(np.sqrt(num_patches))\n",
        "    plt.figure(figsize=(2*n, 2*n))\n",
        "    for i in range(num_patches):\n",
        "        patch = patches[i].permute(1, 2, 0).numpy()\n",
        "        plt.subplot(n, n, i + 1)\n",
        "        plt.imshow(patch)\n",
        "        plt.title(f\"Patch {i+1}\")\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "image_size = 256\n",
        "patch_size = 16\n",
        "\n",
        "image_array = image_preprocessing(\"E:/LangsImgDatasetForDiploma/images_normalized/1000_IM-0003-1001.dcm.png\", image_size) # shape: (image_size, image_size, 3)\n",
        "plt.imshow(image_array)\n",
        "plt.axis('off')  # Turn off axis labels\n",
        "plt.show()\n",
        "input_image = torch.tensor(image_array).permute(2,0,1).unsqueeze(0) # shape: (1, 3, image_size, image_size)\n",
        "input_batch_image = torch.stack([input_image,input_image,input_image,input_image])\n",
        "patches = create_image_patches(input_image, patch_size)\n",
        "print(patches.shape)\n",
        "plot_patches(patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-01-28T06:38:23.443053Z",
          "iopub.status.busy": "2024-01-28T06:38:23.442275Z",
          "iopub.status.idle": "2024-01-28T06:38:23.520999Z",
          "shell.execute_reply": "2024-01-28T06:38:23.520056Z",
          "shell.execute_reply.started": "2024-01-28T06:38:23.44302Z"
        },
        "id": "yMaXj3jnBGvm",
        "papermill": {
          "duration": 0.186855,
          "end_time": "2023-08-30T20:44:04.797561",
          "exception": false,
          "start_time": "2023-08-30T20:44:04.610706",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def create_batch_patches(input_batch_image):\n",
        "    batch_patches = []\n",
        "    for i in range(input_batch_image.shape[0]):\n",
        "        input_image = input_batch_image[i].unsqueeze(0)\n",
        "        patches = create_image_patches(input_image, patch_size)\n",
        "        batch_patches.append(patches)\n",
        "    return torch.stack(batch_patches, dim=0)\n",
        "\n",
        "image_array = image_preprocessing(\"/kaggle/input/chest-xrays-indiana-university/images/images_normalized/1000_IM-0003-2001.dcm.png\", image_size) # shape: (image_size, image_size, 3)\n",
        "input_image = torch.tensor(image_array).permute(2,0,1)\n",
        "input_batch_image = torch.stack([input_image,input_image,input_image,input_image])\n",
        "print(input_batch_image.shape)\n",
        "batch_patches = create_batch_patches(input_batch_image)\n",
        "print(batch_patches.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:38:30.815205Z",
          "iopub.status.busy": "2024-01-28T06:38:30.814594Z",
          "iopub.status.idle": "2024-01-28T06:38:30.845331Z",
          "shell.execute_reply": "2024-01-28T06:38:30.844525Z",
          "shell.execute_reply.started": "2024-01-28T06:38:30.81517Z"
        },
        "id": "WvPgpvqcBGvn",
        "papermill": {
          "duration": 0.184721,
          "end_time": "2023-08-30T20:44:05.13356",
          "exception": false,
          "start_time": "2023-08-30T20:44:04.948839",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.fc_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        Q = self.query(query)\n",
        "        K = self.key(key)\n",
        "        V = self.value(value)\n",
        "\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        attention = torch.softmax(scores, dim=-1)\n",
        "        attended_values = torch.matmul(attention, V)\n",
        "        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        output = self.fc_out(attended_values)\n",
        "        return output, attention\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dim_feedforward, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.fc = nn.Sequential(\n",
        "                                    nn.Linear(d_model, dim_feedforward),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(dim_feedforward, d_model),\n",
        "                                )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attended, attention = self.attention(x, x, x, mask)\n",
        "        x = x + self.dropout1(self.norm1(attended))\n",
        "        feedforward = self.fc(x)\n",
        "        x = x + self.dropout2(self.norm2(feedforward))\n",
        "        return x, attention\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, d_model, n_heads, image_embedding_size, dim_feedforward, num_layers, dropout=0.1):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.patch_dim = 3 * patch_size ** 2\n",
        "        self.patch_embeddings = nn.Linear(self.patch_dim, d_model)\n",
        "        self.position_embeddings = self.generate_positional_encodings(num_patches + 1, d_model) # nn.Parameter(torch.zeros(1, num_patches + 1, d_model))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([TransformerBlock(d_model, n_heads, dim_feedforward, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.fc_head = nn.Linear(d_model, image_embedding_size)\n",
        "        # self.fc = nn.Linear(image_embedding_size, 10)\n",
        "\n",
        "    def generate_positional_encodings(self, num_patches, d_model):\n",
        "        position_encodings = torch.zeros(1, num_patches, d_model)  # Add +1 to num_patches\n",
        "        position = torch.arange(0, num_patches, dtype=torch.float32).unsqueeze(1)  # Add +1 to num_patches\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
        "        position_encodings[:, :, 0::2] = torch.sin(position * div_term)\n",
        "        position_encodings[:, :, 1::2] = torch.cos(position * div_term)\n",
        "        return position_encodings\n",
        "\n",
        "\n",
        "    def create_image_patches(self, image):\n",
        "        batch_size, channels, height, width = image.size()\n",
        "        patches = []\n",
        "        for h in range(0, height, self.patch_size):\n",
        "            for w in range(0, width, self.patch_size):\n",
        "                patch = image[:, :, h:h+self.patch_size, w:w+self.patch_size]\n",
        "                patches.append(patch)\n",
        "        patches = torch.cat(patches, dim=0)\n",
        "        return patches\n",
        "\n",
        "    def create_batch_patches(self,input_batch_image):\n",
        "        batch_patches = []\n",
        "        for i in range(input_batch_image.shape[0]):\n",
        "            input_image = input_batch_image[i].unsqueeze(0)\n",
        "            patches = self.create_image_patches(input_image)\n",
        "            batch_patches.append(patches)\n",
        "        return torch.stack(batch_patches, dim=0).float()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.create_batch_patches(x)\n",
        "        x = x.reshape(x.size(0), x.size(1), x.size(2)*x.size(3)*x.size(4))\n",
        "        x = self.patch_embeddings(x)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.position_embeddings.to(x.device)\n",
        "        for transformer in self.transformer_blocks:\n",
        "            x, _ = transformer(x)\n",
        "\n",
        "        x = x[:, 0]\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc_head(x)\n",
        "        # x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh3gcmGvBGvn"
      },
      "source": [
        "# Checking ViT using classification data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:38:44.054751Z",
          "iopub.status.busy": "2024-01-28T06:38:44.053783Z",
          "iopub.status.idle": "2024-01-28T06:39:06.14731Z",
          "shell.execute_reply": "2024-01-28T06:39:06.146521Z",
          "shell.execute_reply.started": "2024-01-28T06:38:44.054713Z"
        },
        "id": "t5in6_YMBGvn",
        "papermill": {
          "duration": 32.94248,
          "end_time": "2023-08-30T20:44:42.750512",
          "exception": false,
          "start_time": "2023-08-30T20:44:09.808032",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, AutoTokenizer\n",
        "\n",
        "# Load pre-trained DistilBERT model and tokenizer\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name) # DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertModel.from_pretrained(model_name)\n",
        "\n",
        "def generate_text_embeddings(texts):\n",
        "    # Tokenize input texts\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    # Generate embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the embeddings from the last hidden layer\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :] # <---CLS # outputs.last_hidden_state.mean(dim=1)  # You can use other aggregation methods\n",
        "\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:39:09.740363Z",
          "iopub.status.busy": "2024-01-28T06:39:09.739407Z",
          "iopub.status.idle": "2024-01-28T06:39:10.038204Z",
          "shell.execute_reply": "2024-01-28T06:39:10.037161Z",
          "shell.execute_reply.started": "2024-01-28T06:39:09.740329Z"
        },
        "id": "yaGgnRReBGvo",
        "papermill": {
          "duration": 0.281469,
          "end_time": "2023-08-30T20:44:43.18516",
          "exception": false,
          "start_time": "2023-08-30T20:44:42.903691",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# List of input texts\n",
        "texts = [ \"yo yo CLIP Model\", \"DistilBERT is a great model for text embedding.\", \"Vision Transformers for image.\", \"Contrative Learning\" ]\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = generate_text_embeddings(texts)\n",
        "print(\"Shape of embeddings:\", embeddings.shape)  # Should print (num_texts, embedding_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq4hdN2FBGvo",
        "papermill": {
          "duration": 0.155573,
          "end_time": "2023-08-30T20:44:44.825124",
          "exception": false,
          "start_time": "2023-08-30T20:44:44.669551",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:39:15.300325Z",
          "iopub.status.busy": "2024-01-28T06:39:15.299566Z",
          "iopub.status.idle": "2024-01-28T06:39:15.316207Z",
          "shell.execute_reply": "2024-01-28T06:39:15.315125Z",
          "shell.execute_reply.started": "2024-01-28T06:39:15.300292Z"
        },
        "id": "h873DyXoBGvo",
        "papermill": {
          "duration": 0.217704,
          "end_time": "2023-08-30T20:44:45.199842",
          "exception": false,
          "start_time": "2023-08-30T20:44:44.982138",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "dropout = 0.1\n",
        "class ImageProjection(nn.Module):\n",
        "    def __init__(self, image_embedding_size, shared_embedding_size):\n",
        "        super(ImageProjection, self).__init__()\n",
        "        self.image_projection = nn.Linear(image_embedding_size, shared_embedding_size)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(shared_embedding_size, shared_embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(shared_embedding_size)\n",
        "\n",
        "    def forward(self, image_embeddings):\n",
        "        projected_embeddings = self.image_projection(image_embeddings)\n",
        "\n",
        "        x = self.gelu(projected_embeddings)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected_embeddings\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x # projected_embeddings\n",
        "\n",
        "class TextProjection(nn.Module):\n",
        "    def __init__(self, text_embedding_size, shared_embedding_size):\n",
        "        super(TextProjection, self).__init__()\n",
        "        self.text_projection = nn.Linear(text_embedding_size, shared_embedding_size)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(shared_embedding_size, shared_embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(shared_embedding_size)\n",
        "\n",
        "    def forward(self, text_embeddings):\n",
        "        projected_embeddings = self.text_projection(text_embeddings)\n",
        "\n",
        "        x = self.gelu(projected_embeddings)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected_embeddings\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x # projected_embeddings\n",
        "\n",
        "\"\"\"\n",
        "class CosineSimilarity(nn.Module):\n",
        "    def __init__(self, temperature = 0.2 ):\n",
        "        super(CosineSimilarity, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, image_embeddings, text_embeddings):\n",
        "        normalized_images = nn.functional.normalize(image_embeddings, dim=-1)\n",
        "        normalized_texts = nn.functional.normalize(text_embeddings, dim=-1)\n",
        "\n",
        "        similarities = torch.matmul(normalized_images, normalized_texts.t())*self.temperature\n",
        "        return similarities\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:39:18.142301Z",
          "iopub.status.busy": "2024-01-28T06:39:18.141968Z",
          "iopub.status.idle": "2024-01-28T06:39:18.147531Z",
          "shell.execute_reply": "2024-01-28T06:39:18.146536Z",
          "shell.execute_reply.started": "2024-01-28T06:39:18.142277Z"
        },
        "id": "TcmGShOKBGvp",
        "papermill": {
          "duration": 0.171777,
          "end_time": "2023-08-30T20:44:45.843593",
          "exception": false,
          "start_time": "2023-08-30T20:44:45.671816",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def count_trainable_parameters(model):\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return trainable_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:39:19.322391Z",
          "iopub.status.busy": "2024-01-28T06:39:19.32168Z",
          "iopub.status.idle": "2024-01-28T06:39:19.328689Z",
          "shell.execute_reply": "2024-01-28T06:39:19.32771Z",
          "shell.execute_reply.started": "2024-01-28T06:39:19.322358Z"
        },
        "id": "2Q9seleTBGvp"
      },
      "outputs": [],
      "source": [
        "count_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:39:22.961458Z",
          "iopub.status.busy": "2024-01-28T06:39:22.961105Z",
          "iopub.status.idle": "2024-01-28T06:39:23.453374Z",
          "shell.execute_reply": "2024-01-28T06:39:23.452493Z",
          "shell.execute_reply.started": "2024-01-28T06:39:22.961429Z"
        },
        "id": "3Wq8sMkQBGvp",
        "papermill": {
          "duration": 2.82911,
          "end_time": "2023-08-30T20:44:48.836362",
          "exception": false,
          "start_time": "2023-08-30T20:44:46.007252",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the environment variable\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "\n",
        "# Checking Data Setup\n",
        "for batch_idx, (texts, images, labels) in enumerate(train_dataloader):\n",
        "    print( \"batch_idx: \", batch_idx, \" Image Shape: \", images[0].shape, \"Text Count: \", len(texts) )\n",
        "    plt.imshow( torch.moveaxis( images[0], 0, 2 ).numpy() )\n",
        "    plt.show()\n",
        "    print(texts[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u8zYKD3BGvp",
        "papermill": {
          "duration": 0.160869,
          "end_time": "2023-08-30T20:44:49.167244",
          "exception": false,
          "start_time": "2023-08-30T20:44:49.006375",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:39:37.029302Z",
          "iopub.status.busy": "2024-01-28T06:39:37.028945Z",
          "iopub.status.idle": "2024-01-28T06:39:37.038316Z",
          "shell.execute_reply": "2024-01-28T06:39:37.037147Z",
          "shell.execute_reply.started": "2024-01-28T06:39:37.029274Z"
        },
        "id": "7jEIahIrBGvp",
        "papermill": {
          "duration": 0.176948,
          "end_time": "2023-08-30T20:44:50.211814",
          "exception": false,
          "start_time": "2023-08-30T20:44:50.034866",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "dropout = 0.1\n",
        "\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    if reduction == \"none\":\n",
        "        return loss\n",
        "    elif reduction == \"mean\":\n",
        "        return loss.mean()\n",
        "\n",
        "temperature_value = 1\n",
        "\n",
        "def contrastive_clip_loss_function( text_projection,  image_projection, mode=\"eval\" ):\n",
        "    logits = (text_projection @ image_projection.T) / temperature_value\n",
        "    if mode==\"train\":\n",
        "        images_similarity = image_projection @ image_projection.T\n",
        "        texts_similarity = text_projection @ text_projection.T\n",
        "        targets = F.softmax( (images_similarity + texts_similarity) / 2 * temperature_value, dim=-1 )\n",
        "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
        "        return loss.mean()\n",
        "    elif mode==\"eval\":\n",
        "        return logits\n",
        "    else:\n",
        "        print(\"Mention mode\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glWnd4E2BGvq",
        "papermill": {
          "duration": 0.163135,
          "end_time": "2023-08-30T20:44:50.535918",
          "exception": false,
          "start_time": "2023-08-30T20:44:50.372783",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Previous Model Setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:39:40.049482Z",
          "iopub.status.busy": "2024-01-28T06:39:40.048743Z",
          "iopub.status.idle": "2024-01-28T06:39:40.054151Z",
          "shell.execute_reply": "2024-01-28T06:39:40.052954Z",
          "shell.execute_reply.started": "2024-01-28T06:39:40.049443Z"
        },
        "id": "XUy4JGbEBGvq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjR0XszrBGvq"
      },
      "source": [
        "# Training Model - ResNet50 + Distil BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:39:43.920321Z",
          "iopub.status.busy": "2024-01-28T06:39:43.919921Z",
          "iopub.status.idle": "2024-01-28T06:43:23.626829Z",
          "shell.execute_reply": "2024-01-28T06:43:23.625503Z",
          "shell.execute_reply.started": "2024-01-28T06:39:43.920274Z"
        },
        "id": "d5TlAF1kBGvr"
      },
      "outputs": [],
      "source": [
        "\n",
        "#   - - - - - - - - - - - - - - - - - -   Hyperparameters    - - - - - - - - - - - - - - - - - -\n",
        "shared_embedding_size = 512  # d_e shared embedding space\n",
        "learning_rate = 5e-4\n",
        "\n",
        "#  - - - - - - - - - - - - - - - - - -   Create model components   - - - - - - - - - - - - - - - - - -\n",
        "# - - - - - - - - - ResNet Model - - - - - - - - -\n",
        "# Load a pre-trained ResNet model\n",
        "image_embedding_size=1024*2\n",
        "resnet_model = models.resnet50(pretrained=True)\n",
        "# Remove the classification layer (the last fully connected layer)\n",
        "resnet_model = nn.Sequential( *list(resnet_model.children())[:-1] )\n",
        "resnet_model.to(device)\n",
        "print(\" Number of Trainable Parameters in\", \" ResNet50 Model :  \",   count_trainable_parameters(resnet_model))\n",
        "\n",
        "# - - - - - - - - -  Distil BERT Model  - - - - - - - - -\n",
        "max_length = 32\n",
        "text_embedding_size = 768  # d_i\n",
        "\n",
        "# Load pre-trained DistilBERT model and tokenizer\n",
        "model_name = 'distilbert-base-uncased'\n",
        "text_tokenizer = DistilBertTokenizer.from_pretrained(model_name) # .to(device)\n",
        "text_model = DistilBertModel.from_pretrained(model_name ).to(device)\n",
        "print(\" Number of Trainable Parameters in\", \" DistilBERT model :  \",   count_trainable_parameters(text_model))\n",
        "\n",
        "\n",
        "# - - - - - - - - -  Projections  - - - - - - - - -\n",
        "image_projector = ImageProjection(image_embedding_size, shared_embedding_size).to(device)\n",
        "print(\" Number of Trainable Parameters in\", \" Image Projection :  \",   count_trainable_parameters(image_projector))\n",
        "text_projector = TextProjection(text_embedding_size, shared_embedding_size).to(device)\n",
        "print(\" Number of Trainable Parameters in\", \" Text Projection :  \",   count_trainable_parameters(text_projector))\n",
        "print(\"\\n - - - - - - - - - - \\n \\n  Training......  \")\n",
        "\n",
        "# - - - - - - - - -  Loss  - - - - - - - - -\n",
        "# sym_loss_fn = SymmetricalLoss(margin=0.1).to(device)\n",
        "# contrastive_loss = ContrastiveLoss(temperature=0.9)\n",
        "\n",
        "# - - - - - - - - -  Optimizer  - - - - - - - - -\n",
        "# optimizer = optim.AdamW( list(resnet_model.parameters()) + list(text_model.parameters()) + list(image_projector.parameters()) + list(text_projector.parameters()), lr=learning_rate )\n",
        "params = [{\"params\":resnet_model.parameters(), \"lr\":1e-4 }, {\"params\":text_model.parameters(), \"lr\":1e-5},\n",
        "          # {\"params\":image_projector.parameters(), \"lr\": }, {\"params\":text_projector.parameters(), \"lr\":}\n",
        "          {\"params\": itertools.chain( image_projector.parameters(), text_projector.parameters() ), \"lr\":1e-3 , \"weight_decay\":1e-3 }\n",
        "         ]\n",
        "optimizer = optim.AdamW( params,  weight_decay=0. )\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, mode=\"min\", patience=2 , factor=0.8 )\n",
        "\n",
        "# - - - - - - - - -  Training loop  - - - - - - - - -\n",
        "for epoch in range(num_epochs):\n",
        "    # Record the start time\n",
        "    start_time = time.time()\n",
        "    print( \" - - - - - - - - - - - Epoch:\", epoch+1, \" - - - - - - - - - - - - \"  )\n",
        "    resnet_model.train()\n",
        "    text_model.train()\n",
        "    image_projector.train()\n",
        "    text_projector.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, texts) in enumerate(train_dataloader):\n",
        "        # - - - - - - - - -  Forward pass  - - - - - - - - -\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # - - - -  Text  - - - -\n",
        "        inputs = tokenizer(texts, return_tensors='pt', padding=\"max_length\", max_length=max_length, truncation=True)\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = text_model(**inputs)\n",
        "        text_embeddings = outputs.last_hidden_state[:, 0, :] # outputs.last_hidden_state.mean(dim=1)\n",
        "        text_projection = text_projector(text_embeddings)\n",
        "\n",
        "        # - - - -  Image  - - - -\n",
        "        images = images.to(device)\n",
        "        # images = preprocess_images(images)\n",
        "        image_embeddings = resnet_model(images)\n",
        "        image_projection = image_projector(image_embeddings[:,:,0,0])\n",
        "\n",
        "        # - - - -  Compute Loss  - - - -\n",
        "        # loss = sym_loss_fn(image_projection, text_projection)\n",
        "        # loss = contrastive_loss(image_projection, text_projection)\n",
        "        loss = contrastive_clip_loss_function( text_projection,  image_projection, mode=\"train\")\n",
        "\n",
        "        # - - - -  Backpropagation  - - - -\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # - - - - Loss print - - - -\n",
        "        total_loss += loss.item()\n",
        "        if batch_idx % 200 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_dataloader)}], Loss: {total_loss/((batch_idx+1)*batch_size):.4f}\")\n",
        "\n",
        "\n",
        "    lr_scheduler.step(total_loss)\n",
        "\n",
        "    # Record the end time\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    # - - - - Loss each epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\", \"  Time Taken: \", elapsed_time, \" seconds\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T06:43:29.835571Z",
          "iopub.status.busy": "2024-01-28T06:43:29.835181Z"
        },
        "id": "k5QYxrm7BGvr"
      },
      "outputs": [],
      "source": [
        "resnet_model.eval()\n",
        "image_projector.eval()\n",
        "\n",
        "def create_image_embeddings(images):\n",
        "    with torch.no_grad():\n",
        "        image_embeddings = resnet_model(images)\n",
        "        image_projection = image_projector(image_embeddings[:,:,0,0])\n",
        "    return image_projection\n",
        "\n",
        "image_embeddings_list_train = []\n",
        "\n",
        "for index in tqdm(range(len( train_dataset ))):\n",
        "    images = train_dataset[index][0]\n",
        "    images = images.to(device)\n",
        "    image_projection = create_image_embeddings(images.unsqueeze(0))\n",
        "    image_embeddings_list_train.append( image_projection[0] )\n",
        "\n",
        "\n",
        "def image_retrieval_function( input_query, n , display=False): # n --> number of images\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(input_query, return_tensors='pt', padding=\"max_length\", max_length=max_length, truncation=True)\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = text_model(**inputs)\n",
        "        text_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        text_projection = text_projector(text_embeddings)\n",
        "\n",
        "    similarity_scores_list = []\n",
        "    for index in tqdm(range(len(image_embeddings_list_train))):\n",
        "        score = torch.dot( text_projection[0], image_embeddings_list_train[index] )\n",
        "        similarity_scores_list.append( score.cpu().numpy() )\n",
        "\n",
        "    max_indexes = np.array(similarity_scores_list).argsort()[-n:][::-1]\n",
        "    if display:\n",
        "        for index in max_indexes:\n",
        "            image_tensor = train_dataset[index][0]\n",
        "            plt.imshow( torch.moveaxis(image_tensor, 0,2) )\n",
        "            plt.show()\n",
        "        return None\n",
        "    else:\n",
        "        return max_indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4iYKDoGBGvr"
      },
      "outputs": [],
      "source": [
        "input_query = \"silhouette\"\n",
        "image_retrieval_function( input_query, n=10, display=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0RZSbU3BGvs"
      },
      "outputs": [],
      "source": [
        "input_query = \"man\"\n",
        "image_retrieval_function( input_query, n=3, display=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J1r3kEjBGvs"
      },
      "outputs": [],
      "source": [
        "input_query = \"women\"\n",
        "image_retrieval_function( input_query, n=5, display=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "id": "dBLjVLyhBGvs"
      },
      "outputs": [],
      "source": [
        "input_query = \"bike on the road\"\n",
        "image_retrieval_function( input_query, n=2, display=True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU0zjR4XBGvw"
      },
      "source": [
        "# Training Model - ViT + Distil BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbIsa4iMBGvx"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVP_sVd9BGvx"
      },
      "outputs": [],
      "source": [
        "#   - - - - - - - - - - - - - - - - - -   Hyperparameters    - - - - - - - - - - - - - - - - - -\n",
        "shared_embedding_size = 512  # d_e shared embedding space\n",
        "\n",
        "#  - - - - - - - - - - - - - - - - - -   Create model components   - - - - - - - - - - - - - - - - - -\n",
        "# - - - - - - - - - ViT Model - - - - - - - - -\n",
        "image_size = 256\n",
        "patch_size = 16\n",
        "d_model = 512*4\n",
        "n_heads = 8\n",
        "dim_feedforward = 2048\n",
        "num_layers = 12\n",
        "image_embedding_size=1024  # d_i\n",
        "\n",
        "ViT_model = VisionTransformer(image_size, patch_size, d_model, n_heads, image_embedding_size, dim_feedforward, num_layers).to(device)\n",
        "print(\" Number of Trainable Parameters in\", \" ViT model :  \",   count_trainable_parameters(ViT_model))\n",
        "\n",
        "# - - - - - - - - -  Distil BERT Model  - - - - - - - - -\n",
        "max_length = 32\n",
        "text_embedding_size = 768  # d_i\n",
        "\n",
        "# Load pre-trained DistilBERT model and tokenizer\n",
        "model_name = 'distilbert-base-uncased'\n",
        "text_tokenizer = DistilBertTokenizer.from_pretrained(model_name) # .to(device)\n",
        "text_model = DistilBertModel.from_pretrained(model_name ).to(device)\n",
        "print(\" Number of Trainable Parameters in\", \" DistilBERT model :  \",   count_trainable_parameters(text_model))\n",
        "\n",
        "\n",
        "# - - - - - - - - -  Projections  - - - - - - - - -\n",
        "image_projector = ImageProjection(image_embedding_size, shared_embedding_size).to(device)\n",
        "print(\" Number of Trainable Parameters in\", \" Image Projection :  \",   count_trainable_parameters(image_projector))\n",
        "text_projector = TextProjection(text_embedding_size, shared_embedding_size).to(device)\n",
        "print(\" Number of Trainable Parameters in\", \" Text Projection :  \",   count_trainable_parameters(text_projector))\n",
        "print(\"\\n - - - - - - - - - - \\n \\n  Training......  \")\n",
        "\n",
        "# - - - - - - - - -  Loss  - - - - - - - - -\n",
        "# sym_loss_fn = SymmetricalLoss(margin=0.1).to(device)\n",
        "# contrastive_loss = ContrastiveLoss(temperature=0.9)\n",
        "\n",
        "# - - - - - - - - -  Optimizer  - - - - - - - - -\n",
        "# optimizer = optim.AdamW( list(ViT_model.parameters())+ list(text_model.parameters()) + list(image_projector.parameters()) + list(text_projector.parameters()), lr=learning_rate )\n",
        "params = [{\"params\":ViT_model.parameters(), \"lr\":1e-3 }, {\"params\":text_model.parameters(), \"lr\":1e-4},\n",
        "          # {\"params\":image_projector.parameters(), \"lr\": }, {\"params\":text_projector.parameters(), \"lr\":}\n",
        "          {\"params\": itertools.chain( image_projector.parameters(), text_projector.parameters() ), \"lr\":1e-3 , \"weight_decay\":1e-3 }\n",
        "         ]\n",
        "optimizer = optim.AdamW( params,  weight_decay=0.0 )\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, mode=\"min\", patience=1 , factor=0.8 )\n",
        "\n",
        "# import torch.optim.lr_scheduler as lr_scheduler\n",
        "# scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5) # step_size=step_size\n",
        "# warmup_epochs = 25  # Number of epochs for warm-up\n",
        "\n",
        "# - - - - - - - - -  Training loop  - - - - - - - - -\n",
        "for epoch in range(num_epochs):\n",
        "    print( \" - - - - - - - - - - - Epoch:\", epoch+1, \" - - - - - - - - - - - - \"  )\n",
        "    ViT_model.train()\n",
        "    text_model.train()\n",
        "    image_projector.train()\n",
        "    text_projector.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, texts) in enumerate(train_dataloader):\n",
        "        # - - - - - - - - -  Forward pass  - - - - - - - - -\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # - - - -  Text  - - - -\n",
        "        inputs = tokenizer(texts, return_tensors='pt', padding=\"max_length\", max_length=max_length, truncation=True)\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = text_model(**inputs)\n",
        "        text_embeddings = outputs.last_hidden_state[:, 0, :] # outputs.last_hidden_state.mean(dim=1)\n",
        "        text_projection = text_projector(text_embeddings)\n",
        "\n",
        "        # - - - -  Image  - - - -\n",
        "        images = images.to(device)\n",
        "        image_embeddings = ViT_model(images)\n",
        "        image_projection = image_projector(image_embeddings)\n",
        "\n",
        "        # - - - -  Compute Loss  - - - -\n",
        "        # loss = sym_loss_fn(image_projection, text_projection)\n",
        "        # loss = contrastive_loss(image_projection, text_projection)\n",
        "        loss = contrastive_clip_loss_function( text_projection,  image_projection, mode=\"train\")\n",
        "\n",
        "        # - - - -  Backpropagation  - - - -\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "#     if epoch < warmup_epochs:\n",
        "#         scheduler.step(epoch)\n",
        "#     else:\n",
        "#         scheduler.step()\n",
        "\n",
        "        # - - - - Loss print - - - -\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 200 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_dataloader)}], Loss: {total_loss/((batch_idx+1)*batch_size):.4f}\")\n",
        "\n",
        "\n",
        "    # - - - - Loss each epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    lr_scheduler.step(avg_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "id": "hW7F30p8BGvx",
        "papermill": {
          "duration": 5.371338,
          "end_time": "2023-08-31T05:57:45.42658",
          "exception": false,
          "start_time": "2023-08-31T05:57:40.055242",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# 11891904 + 22155456 + 66362880 + 105987072\n",
        "\n",
        "try:\n",
        "    checkpoint = {\n",
        "        'ViT_model_dict': ViT_model.state_dict(),\n",
        "        # 'resnet_model_dict': resnet_model.state_dict(),\n",
        "        'text_model_dict': text_model.state_dict(),\n",
        "        'image_projector_dict': image_projector.state_dict(),\n",
        "        'text_projector_dict': text_projector.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, \"/kaggle/working/CLIP_model_from_Scratch_ResNet_DistilBERT\")\n",
        "\n",
        "except:\n",
        "    print(\"Error in some saving\")\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOPqT3yUBGvx"
      },
      "source": [
        "# Inference | Image Retrieval | ViT Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "id": "xclKkfniBGvy",
        "papermill": {
          "duration": 132.447943,
          "end_time": "2023-08-31T06:00:05.398792",
          "exception": false,
          "start_time": "2023-08-31T05:57:52.950849",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "ViT_model.eval()\n",
        "image_projector.eval()\n",
        "\n",
        "def create_image_embeddings(images):\n",
        "    with torch.no_grad():\n",
        "        image_embeddings = ViT_model(images)\n",
        "        image_projection = image_projector(image_embeddings)\n",
        "    return image_projection\n",
        "\n",
        "image_embeddings_list_train = []\n",
        "\n",
        "for index in tqdm(range(len( train_dataset ))):\n",
        "    images = train_dataset[index][0]\n",
        "    images = images.to(device)\n",
        "    image_projection = create_image_embeddings(images.unsqueeze(0))\n",
        "    image_embeddings_list_train.append( image_projection[0] )\n",
        "\n",
        "\n",
        "def image_retrieval_function( input_query, n , display=False): # n --> number of images\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(input_query, return_tensors='pt', padding=\"max_length\", max_length=max_length, truncation=True)\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = text_model(**inputs)\n",
        "        text_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        text_projection = text_projector(text_embeddings)\n",
        "\n",
        "    similarity_scores_list = []\n",
        "    for index in tqdm(range(len(image_embeddings_list_train))):\n",
        "        score = torch.dot( text_projection[0], image_embeddings_list_train[index] )\n",
        "        similarity_scores_list.append( score.cpu().numpy() )\n",
        "\n",
        "    max_indexes = np.array(similarity_scores_list).argsort()[-n:][::-1]\n",
        "    if display:\n",
        "        for index in max_indexes:\n",
        "            image_tensor = train_dataset[index][0]\n",
        "            plt.imshow( torch.moveaxis(image_tensor, 0,2) )\n",
        "            plt.show()\n",
        "        return None\n",
        "    else:\n",
        "        return max_indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": false,
        "id": "2eGFHLbvBGvy",
        "papermill": {
          "duration": 1.761107,
          "end_time": "2023-08-31T06:00:07.484356",
          "exception": false,
          "start_time": "2023-08-31T06:00:05.723249",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "input_query = \"dog\"\n",
        "image_retrieval_function( input_query, n=3, display=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "id": "BB8QFflLBGvy",
        "papermill": {
          "duration": 1.691914,
          "end_time": "2023-08-31T06:00:09.508613",
          "exception": false,
          "start_time": "2023-08-31T06:00:07.816699",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "input_query = \"man\"\n",
        "image_retrieval_function( input_query, n=3, display=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "id": "_vy-bpu1BGvz",
        "papermill": {
          "duration": 2.273767,
          "end_time": "2023-08-31T06:00:12.128153",
          "exception": false,
          "start_time": "2023-08-31T06:00:09.854386",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "input_query = \"women\"\n",
        "image_retrieval_function( input_query, n=5, display=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "id": "PG5KWeTmBGvz",
        "papermill": {
          "duration": 2.725346,
          "end_time": "2023-08-31T06:00:15.315965",
          "exception": false,
          "start_time": "2023-08-31T06:00:12.590619",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "input_query = \"bike on the road\"\n",
        "image_retrieval_function( input_query, n=5, display=True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_kg_hide-input": true,
        "id": "WwyU7_5gBGvz",
        "papermill": {
          "duration": 0.356117,
          "end_time": "2023-08-31T06:00:16.039553",
          "exception": false,
          "start_time": "2023-08-31T06:00:15.683436",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Evaluation | Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": false,
        "id": "InNh_icMBGvz",
        "papermill": {
          "duration": 100.010034,
          "end_time": "2023-08-31T06:01:56.405376",
          "exception": false,
          "start_time": "2023-08-31T06:00:16.395342",
          "status": "completed"
        },
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "accuracy_counter = 0\n",
        "\n",
        "# ViT_model.eval()\n",
        "resnet_model.eval()\n",
        "text_model.eval()\n",
        "image_projector.eval()\n",
        "text_projector.eval()\n",
        "\n",
        "# cosine_similarity = CosineSimilarity(temperature=temperature_value)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_images, batch_captions in tqdm(train_dataloader):\n",
        "        batch_size = batch_images.size(0)\n",
        "\n",
        "        # - - - - - - - - -  Forward pass  - - - - - - - - -\n",
        "        # - - - -  Text  - - - -\n",
        "        inputs = tokenizer(batch_captions, return_tensors='pt', padding=\"max_length\", max_length=max_length, truncation=True)\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = text_model(**inputs)\n",
        "        text_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        text_projection = text_projector(text_embeddings)\n",
        "\n",
        "        # - - - -  Image  - - - -\n",
        "        batch_images = batch_images.to(device)\n",
        "#         image_embeddings = ViT_model(batch_images)\n",
        "#         image_projection = image_projector(image_embeddings)\n",
        "        image_embeddings = resnet_model(batch_images)\n",
        "        image_projection = image_projector(image_embeddings[:,:,0,0])\n",
        "\n",
        "        # Calculate cosine similarities\n",
        "        for index_text in range(batch_size):\n",
        "            similarity_scores_list = []\n",
        "            for index_image in range(len(image_projection)):\n",
        "                score = torch.dot( text_projection[index_text], image_projection[index_image] )\n",
        "                similarity_scores_list.append( score.cpu().numpy() )\n",
        "            similarity_scores_list = np.array(similarity_scores_list)\n",
        "\n",
        "            max_index = np.argmax(similarity_scores_list)\n",
        "            if max_index==index_text:\n",
        "                accuracy_counter += 1\n",
        "\n",
        "#         # Convert indices to captions for comparison\n",
        "#         batch_pred_captions = [batch_captions[index] for index in batch_pred_indices]\n",
        "#         batch_actual_captions = batch_captions\n",
        "\n",
        "#         # Calculate accuracy\n",
        "#         for pred_caption, actual_caption in zip(batch_pred_captions, batch_actual_captions):\n",
        "#             if pred_caption == actual_caption:\n",
        "#                 accuracy_counter += 1\n",
        "\n",
        "total_samples = len(train_dataloader.dataset)\n",
        "accuracy = accuracy_counter / total_samples\n",
        "print(f\"Accuracy (Pencentage of Correct Matching): {accuracy*100:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": false,
        "id": "iHzqE1FPBGv0",
        "papermill": {
          "duration": 99.963085,
          "end_time": "2023-08-31T06:03:36.773355",
          "exception": false,
          "start_time": "2023-08-31T06:01:56.81027",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "accuracy_counter = 0\n",
        "\n",
        "# ViT_model.eval()\n",
        "resnet_model.eval()\n",
        "text_model.eval()\n",
        "image_projector.eval()\n",
        "text_projector.eval()\n",
        "\n",
        "# cosine_similarity = CosineSimilarity(temperature=temperature_value)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_images, batch_captions in tqdm(test_dataloader):\n",
        "        batch_size = batch_images.size(0)\n",
        "\n",
        "        # - - - - - - - - -  Forward pass  - - - - - - - - -\n",
        "        # - - - -  Text  - - - -\n",
        "        inputs = tokenizer(batch_captions, return_tensors='pt', padding=\"max_length\", max_length=max_length, truncation=True)\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = text_model(**inputs)\n",
        "        text_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        text_projection = text_projector(text_embeddings)\n",
        "\n",
        "        # - - - -  Image  - - - -\n",
        "        batch_images = batch_images.to(device)\n",
        "        image_embeddings = resnet_model(batch_images)\n",
        "        image_projection = image_projector(image_embeddings[:,:,0,0])\n",
        "\n",
        "        # Calculate cosine similarities\n",
        "        for index_text in range(batch_size):\n",
        "            similarity_scores_list = []\n",
        "            for index_image in range(len(image_projection)):\n",
        "                score = torch.dot( text_projection[index_text], image_projection[index_image] )\n",
        "                similarity_scores_list.append( score.cpu().numpy() )\n",
        "            similarity_scores_list = np.array(similarity_scores_list)\n",
        "\n",
        "            max_index = np.argmax(similarity_scores_list)\n",
        "            if max_index==index_text:\n",
        "                accuracy_counter += 1\n",
        "\n",
        "total_samples = len(test_dataloader.dataset)\n",
        "accuracy = accuracy_counter / total_samples\n",
        "print(f\"Accuracy (Pencentage of Correct Matching): {accuracy*100:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odkQh1-pBGv0",
        "papermill": {
          "duration": 0.436423,
          "end_time": "2023-08-31T06:03:38.533049",
          "exception": false,
          "start_time": "2023-08-31T06:03:38.096626",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# References:\n",
        "\n",
        "Image Reference: https://www.youtube.com/watch?v=GLa7z5rkSf4\n",
        "\n",
        "Paper: https://arxiv.org/abs/2103.00020\n",
        "\n",
        "Website: https://openai.com/research/clip\n",
        "\n",
        "Vision Transformer Model (ViT) : https://arxiv.org/pdf/2010.11929v2.pdf  |  https://huggingface.co/docs/transformers/main/model_doc/vit"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 516716,
          "sourceId": 951996,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30528,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}